{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3204401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpc/anaconda3/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from openai import api_key, base_url\n",
    "from transformers import Qwen2_5_VLConfig\n",
    "\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain.schema import output_parser\n",
    "# llm = ChatOpenAI(model=)\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"sk-C3gsaWJaOsmjrNv1fIobc4zEEsEkD15T98j80Vc8mNuSE420\")\n",
    "# llm = ChatOpenAI(model=\"o3-mini\", api_key=OPENAI_API_KEY, base_url=\"https://api.deepbricks.ai/v1/\")\n",
    "QWEN_API_KEY = os.getenv(\"QWEN_API_KEY\", \"sk-ypjnechwutigvbyglbkguukzsmzkkxfibauydwkbjrypwojd\")\n",
    "model = ChatOpenAI(model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\", api_key=QWEN_API_KEY, base_url=\"https://api.siliconflow.cn/v1\")\n",
    "\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febe6b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_144345/4018669994.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_path)\n"
     ]
    }
   ],
   "source": [
    "##RAG\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_path = \"/home/zpc/langchain/blibli/bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54b1d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7faa0d410c70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"小明在华为工作\", \"熊喜欢吃蜂蜜\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6e7238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d763e52e-f521-4d01-af84-fc2706869c56', metadata={}, page_content='熊喜欢吃蜂蜜'),\n",
       " Document(id='6ca00a0f-56c5-4001-8a91-2d403b1e8c4b', metadata={}, page_content='小明在华为工作')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = vectorstore.as_retriever()\n",
    "retriver.invoke(\"熊喜欢吃什么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674c0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "只根据以下文档回答问题\n",
    "{context}\n",
    "问题：{question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c01ac5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "outputParser = StrOutputParser()\n",
    "setup_and_retrival = RunnableParallel({\n",
    "    \"context\":retriver,\n",
    "    \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chain = setup_and_retrival | prompt | model | outputParser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77ca1e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小明在华为工作。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"小明在哪里工作\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "327d28db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴迪哥，小明在华为工作。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "template = \"\"\"\n",
    "只根据以下文档回答\n",
    "{context}\n",
    "问题: {question},\n",
    "回答问题请带上称呼:{name}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriver,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"name\": itemgetter(\"name\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chain.invoke({\"question\": \"小明在哪里工作\", \"name\": \"吴迪哥\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f2ebeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/zpc/langchain/blibli/data/faq_1.txt'}, page_content='一、什么是0分期利息\\n\\n您好，“0分期利息”是指买家使用花呗、招行掌上生活、工行信用卡、银联信用卡等其他分期购物时无需支付分期利息的功能，分期利息由华为商城承担。\\n\\n注：自2023年起，商城将相关宣传将“免息”调整为“0分期利息”，主要基于中国银保监会、中国人民银行《关于进一步促进信用卡业务规范健康发展的通知》（银保监规〔2022〕13号），要求“银行业金融机构应当在分期业务合同（协议）首页和业务办理页面以明显方式展示分期业务可能产生的所有息费项目、年化利率水平和息费计算方式。向客户展示分期业务收取的资金使用成本时，应当统一采用利息形式，并明确相应的计息规则，不得采用手续费等形式，法律法规另有规定的除外。”\\n\\n\\n\\n二、可以参与0分期利息活动的商品\\n\\n商城目前仅支持部分单品参与0分期利息，若多商品（含不支持0分期利息）合并支付则不支持0分期利息，以支付页面为准，后续会逐渐开放更多商品，敬请关注。\\n\\n\\n\\n三、确认订单分期成功\\n\\n订单提交成功后在支付方式页面选择分期支付，点选显示0分期利息的支付方式及具体0分期利息期数后，完成支付。\\n\\n\\n\\n\\n\\n四、订单中有多个商品，其中有商品支持0分期利息，为什么提交后却没有0分期利息？\\n\\n0分期利息商品不能和其它商品一起购买，如果和其他商品购买而不能享用0分期利息，建议取消原来的订单，重新购买时把0分期利息商品和其他商品分开单独购买；且0分期利息的分期数是固定的，如6期0分期利息，并不是3/6/12都提供0分期利息的。\\n\\n\\n\\n五、小程序是否支持0分期利息？\\n\\n华为商城小程序暂不支持0分期利息。\\n')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader =  TextLoader(\"/home/zpc/langchain/blibli/data/faq_1.txt\")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46208b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/zpc/langchain/blibli/data/faq_2.txt'}, page_content='众测活动\\n\\n整体介绍：\\n\\n一、活动定义：众测是以低价试销的形式，通过收集评价、销量等方法，用于测试市场对新商品的反应，用于及时优化销售策略和引导商家改进。\\n\\n二、优势：众测价通常比较优惠，以不高于大促促销价为原则，最终以与物权方谈判结果为准。\\n\\n三、适用范围：华为商城所有产品。\\n\\n\\n\\n参与方式：\\n\\n1、华为商城众测的入口在哪里？\\n\\n华为商城搜索“众测”，即可看到众测入口，点击进入即可；\\n\\n2、众测上新频次：\\n\\n众测频道每周一至周五不定期更新上架，部分商品可能会特别调整；\\n\\n3、众测活动时间：\\n\\n一款产品众测时间通常为10-20天（热销的商品可能会延期5-10天）。\\n\\n\\n\\n常见问题：\\n\\n1、众测商品的来源?\\n\\n     您好，众测商品主要来源为华为商城上架的新品，热销的爆款商品也会不定期通过众测回归上线。\\n\\n2、众测商品的价格会优惠吗？\\n\\n     您好，众测商品价格对标618和双十一，一般都是该商品某段时间的最低促销价。\\n\\n3、 众测商品质量会不稳定么？\\n\\n     您好，众测商品也是量产的正品新品，质量与正式上架商品一致。\\n\\n4、众测商品买下后多久发货？\\n\\n     您好，请以商品页显示为准。\\n\\n5、提交活动订单后多久内支付？\\n\\n     您好，提交订单后最长付款时效为24小时，逾期订单自动取消\\n\\n6、成功下单后怎么查询众测订单？\\n\\n     您好，成功下单后，您可以通过华为商城手机APP、华为商城手机WAP版、华为商城电脑网页版任意一端登录下单账户，在“我的订单”查询。（众测商品订单查询方式等同于正常商品订单）\\n\\n7、订单支付后未发货前可以取消订单吗？\\n\\n     您好，在发货前消费者可以取消订单。\\n\\n8、取消订单后多久内退回款项？\\n\\n     您好，和华为商城智能家居生态产品通用退款时效一致，3-5个工作日。\\n\\n9、一个账号可以参与几次众测活动？有限购吗？\\n\\n     您好，一个账号参与众测活动的次数无限制，没有限购。但每次订单只能购买一个商品。如您需要购买多个产品，可以多次参与众测活动。\\n')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader1 = TextLoader(\"/home/zpc/langchain/blibli/data/faq_2.txt\")\n",
    "doc1 = loader1.load()\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0616bf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/zpc/langchain/blibli/data/yjhx.md'}, page_content='**一、选择回收物品**\\n\\n点击VMALL您可以在设备回收分类中选择旧商品的类别以及品牌，现在VMALL支持回收部分品牌和型号的手机、笔记本、平板、智能数码以及摄影摄像。如若在此页面上找不到您的品牌或型号，您可以点击品牌栏右下方的加号（如下图红圈部分所示）查看VMALL所支持回收的所有品牌型号。首页滚动电子横幅右下方“以旧换新”标志（如下图红框部分所示），进入“以旧换新”页面。\\n\\n![以旧换新图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/73564551948615546537.png)\\n\\n（以下图片为参考流程，实际以页面为准）\\n\\n![设备回收图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/18969746813614796981.png)\\n\\n选择好您的旧商品后请点击商品图标，进入价格评估页面。\\n\\n\\n\\n**二、价格评估**\\n\\n在价格评估界面上，您需要对旧商品的基本信息（网络制式、存储容量、购买渠道、机身颜色），外观成色（边框背板，屏幕外观，屏幕显示，维修情况），和多功能选项参数进行评估，请如实填写您的手机情况。完成所有参数选择，请估价按钮（如下图红框部分所示）。\\n\\n完成所有参数选择，请点击“免费询价”按钮（如下图红框部分所示）。\\n\\n![免费询价图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/37174946813614947173.png)\\n\\n**三、价格评估结果**\\n\\n您可以查看手机的估价和这款手机的回收价格趋势，如果填写有什么问题，可以点击“重新询价”或“重新选择机型”更改；确认无误后，点击“换钱”按钮进入订单页面。\\n\\n![价格评估结果图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/28422256813615222482.png)\\n\\n**四、提交订单**\\n\\n在订单页面，您可以从以下两种常规回收方式选择一种进行回收，并提交订单结账。（回收宝支持顺丰上门、上门质检回收，爱回收支持顺丰上门、质检上门、信用回收）\\n\\n![&#x56fe;5.png](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/47605356813615350674.png)\\n\\n（1）顺丰上门回收操作流程\\n\\n① 回收方式：选择顺丰快递回收\\n\\n② 提交订单：填写联系人、电话号码、取货地址及上门时间，提交订单\\n\\n③ 上门取货：回收订单提交成功，快递员会按照填写的上门时间上门取货\\n\\n④ 确认交易：回收商会在收到旧机的48小时内完成检测，若价格与您提交订单时的价格一致，将立即发放华为代金券。\\n\\n（2）质检师上门回收操作流程\\n\\n① 回收方式：选择质检师上门回收。\\n\\n② 提交订单：填写联系人信息、手机号码、上门地址及上门时间，提交订单\\n\\n③ 确认交易：如通过质检师上门回收，在完成检测后，代金券可实时到账。\\n\\n![质检师上门回收操作流程图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/67845656813615654876.png)\\n\\n\\n\\n五、以旧换新答疑 ：更多关于以旧换新的问题请点击 “[以旧换新答疑](https://www.vmall.com/help/category-251.html)”查看。\\n')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader3 = TextLoader(\"/home/zpc/langchain/blibli/data/yjhx.md\", encoding=\"utf-8\")\n",
    "doc3 = loader3.load()\n",
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e510752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7fa9a9ae8f40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector, SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_path = \"/home/zpc/langchain/blibli/bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_path)\n",
    "\n",
    "vectorStoreDB = FAISS.from_documents(documents=[doc[0], doc1[0], doc3[0]], embedding=embeddings)\n",
    "\n",
    "\n",
    "vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c4b7b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='4488d4cc-030b-45c9-98be-b31bed0e4427', metadata={'source': '/home/zpc/langchain/blibli/data/yjhx.md'}, page_content='**一、选择回收物品**\\n\\n点击VMALL您可以在设备回收分类中选择旧商品的类别以及品牌，现在VMALL支持回收部分品牌和型号的手机、笔记本、平板、智能数码以及摄影摄像。如若在此页面上找不到您的品牌或型号，您可以点击品牌栏右下方的加号（如下图红圈部分所示）查看VMALL所支持回收的所有品牌型号。首页滚动电子横幅右下方“以旧换新”标志（如下图红框部分所示），进入“以旧换新”页面。\\n\\n![以旧换新图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/73564551948615546537.png)\\n\\n（以下图片为参考流程，实际以页面为准）\\n\\n![设备回收图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/18969746813614796981.png)\\n\\n选择好您的旧商品后请点击商品图标，进入价格评估页面。\\n\\n\\n\\n**二、价格评估**\\n\\n在价格评估界面上，您需要对旧商品的基本信息（网络制式、存储容量、购买渠道、机身颜色），外观成色（边框背板，屏幕外观，屏幕显示，维修情况），和多功能选项参数进行评估，请如实填写您的手机情况。完成所有参数选择，请估价按钮（如下图红框部分所示）。\\n\\n完成所有参数选择，请点击“免费询价”按钮（如下图红框部分所示）。\\n\\n![免费询价图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/37174946813614947173.png)\\n\\n**三、价格评估结果**\\n\\n您可以查看手机的估价和这款手机的回收价格趋势，如果填写有什么问题，可以点击“重新询价”或“重新选择机型”更改；确认无误后，点击“换钱”按钮进入订单页面。\\n\\n![价格评估结果图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/28422256813615222482.png)\\n\\n**四、提交订单**\\n\\n在订单页面，您可以从以下两种常规回收方式选择一种进行回收，并提交订单结账。（回收宝支持顺丰上门、上门质检回收，爱回收支持顺丰上门、质检上门、信用回收）\\n\\n![&#x56fe;5.png](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/47605356813615350674.png)\\n\\n（1）顺丰上门回收操作流程\\n\\n① 回收方式：选择顺丰快递回收\\n\\n② 提交订单：填写联系人、电话号码、取货地址及上门时间，提交订单\\n\\n③ 上门取货：回收订单提交成功，快递员会按照填写的上门时间上门取货\\n\\n④ 确认交易：回收商会在收到旧机的48小时内完成检测，若价格与您提交订单时的价格一致，将立即发放华为代金券。\\n\\n（2）质检师上门回收操作流程\\n\\n① 回收方式：选择质检师上门回收。\\n\\n② 提交订单：填写联系人信息、手机号码、上门地址及上门时间，提交订单\\n\\n③ 确认交易：如通过质检师上门回收，在完成检测后，代金券可实时到账。\\n\\n![质检师上门回收操作流程图片](https://threejs-1251830808.cos.ap-guangzhou.myqcloud.com/67845656813615654876.png)\\n\\n\\n\\n五、以旧换新答疑 ：更多关于以旧换新的问题请点击 “[以旧换新答疑](https://www.vmall.com/help/category-251.html)”查看。\\n'),\n",
       "  np.float32(0.8209087)),\n",
       " (Document(id='9ee5607a-09e0-43cb-9cdd-3ccb8fa46d9c', metadata={'source': '/home/zpc/langchain/blibli/data/faq_2.txt'}, page_content='众测活动\\n\\n整体介绍：\\n\\n一、活动定义：众测是以低价试销的形式，通过收集评价、销量等方法，用于测试市场对新商品的反应，用于及时优化销售策略和引导商家改进。\\n\\n二、优势：众测价通常比较优惠，以不高于大促促销价为原则，最终以与物权方谈判结果为准。\\n\\n三、适用范围：华为商城所有产品。\\n\\n\\n\\n参与方式：\\n\\n1、华为商城众测的入口在哪里？\\n\\n华为商城搜索“众测”，即可看到众测入口，点击进入即可；\\n\\n2、众测上新频次：\\n\\n众测频道每周一至周五不定期更新上架，部分商品可能会特别调整；\\n\\n3、众测活动时间：\\n\\n一款产品众测时间通常为10-20天（热销的商品可能会延期5-10天）。\\n\\n\\n\\n常见问题：\\n\\n1、众测商品的来源?\\n\\n     您好，众测商品主要来源为华为商城上架的新品，热销的爆款商品也会不定期通过众测回归上线。\\n\\n2、众测商品的价格会优惠吗？\\n\\n     您好，众测商品价格对标618和双十一，一般都是该商品某段时间的最低促销价。\\n\\n3、 众测商品质量会不稳定么？\\n\\n     您好，众测商品也是量产的正品新品，质量与正式上架商品一致。\\n\\n4、众测商品买下后多久发货？\\n\\n     您好，请以商品页显示为准。\\n\\n5、提交活动订单后多久内支付？\\n\\n     您好，提交订单后最长付款时效为24小时，逾期订单自动取消\\n\\n6、成功下单后怎么查询众测订单？\\n\\n     您好，成功下单后，您可以通过华为商城手机APP、华为商城手机WAP版、华为商城电脑网页版任意一端登录下单账户，在“我的订单”查询。（众测商品订单查询方式等同于正常商品订单）\\n\\n7、订单支付后未发货前可以取消订单吗？\\n\\n     您好，在发货前消费者可以取消订单。\\n\\n8、取消订单后多久内退回款项？\\n\\n     您好，和华为商城智能家居生态产品通用退款时效一致，3-5个工作日。\\n\\n9、一个账号可以参与几次众测活动？有限购吗？\\n\\n     您好，一个账号参与众测活动的次数无限制，没有限购。但每次订单只能购买一个商品。如您需要购买多个产品，可以多次参与众测活动。\\n'),\n",
       "  np.float32(1.2463478)),\n",
       " (Document(id='420101aa-343c-4586-93bc-e64d0d60b86a', metadata={'source': '/home/zpc/langchain/blibli/data/faq_1.txt'}, page_content='一、什么是0分期利息\\n\\n您好，“0分期利息”是指买家使用花呗、招行掌上生活、工行信用卡、银联信用卡等其他分期购物时无需支付分期利息的功能，分期利息由华为商城承担。\\n\\n注：自2023年起，商城将相关宣传将“免息”调整为“0分期利息”，主要基于中国银保监会、中国人民银行《关于进一步促进信用卡业务规范健康发展的通知》（银保监规〔2022〕13号），要求“银行业金融机构应当在分期业务合同（协议）首页和业务办理页面以明显方式展示分期业务可能产生的所有息费项目、年化利率水平和息费计算方式。向客户展示分期业务收取的资金使用成本时，应当统一采用利息形式，并明确相应的计息规则，不得采用手续费等形式，法律法规另有规定的除外。”\\n\\n\\n\\n二、可以参与0分期利息活动的商品\\n\\n商城目前仅支持部分单品参与0分期利息，若多商品（含不支持0分期利息）合并支付则不支持0分期利息，以支付页面为准，后续会逐渐开放更多商品，敬请关注。\\n\\n\\n\\n三、确认订单分期成功\\n\\n订单提交成功后在支付方式页面选择分期支付，点选显示0分期利息的支付方式及具体0分期利息期数后，完成支付。\\n\\n\\n\\n\\n\\n四、订单中有多个商品，其中有商品支持0分期利息，为什么提交后却没有0分期利息？\\n\\n0分期利息商品不能和其它商品一起购买，如果和其他商品购买而不能享用0分期利息，建议取消原来的订单，重新购买时把0分期利息商品和其他商品分开单独购买；且0分期利息的分期数是固定的，如6期0分期利息，并不是3/6/12都提供0分期利息的。\\n\\n\\n\\n五、小程序是否支持0分期利息？\\n\\n华为商城小程序暂不支持0分期利息。\\n'),\n",
       "  np.float32(1.3561194))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorStoreDB.similarity_search_with_score(\"回收手机应该怎么办?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdba3ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zpc/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "887b50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 27.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/zpc/langchain/blibli/data/faq_2.txt'}, page_content='众测活动\\n\\n整体介绍：\\n\\n一、活动定义：众测是以低价试销的形式，通过收集评价、销量等方法，用于测试市场对新商品的反应，用于及时优化销售策略和引导商家改进。\\n\\n二、优势：众测价通常比较优惠，以不高于大促促销价为原则，最终以与物权方谈判结果为准。\\n\\n三、适用范围：华为商城所有产品。\\n\\n参与方式：\\n\\n1、华为商城众测的入口在哪里？\\n\\n华为商城搜索“众测”，即可看到众测入口，点击进入即可；\\n\\n2、众测上新频次：\\n\\n众测频道每周一至周五不定期更新上架，部分商品可能会特别调整；\\n\\n3、众测活动时间：\\n\\n一款产品众测时间通常为10\\n\\n20天（热销的商品可能会延期5\\n\\n10天）。\\n\\n常见问题：\\n\\n1、众测商品的来源?\\n\\n您好，众测商品主要来源为华为商城上架的新品，热销的爆款商品也会不定期通过众测回归上线。\\n\\n2、众测商品的价格会优惠吗？\\n\\n您好，众测商品价格对标618和双十一，一般都是该商品某段时间的最低促销价。\\n\\n3、 众测商品质量会不稳定么？\\n\\n您好，众测商品也是量产的正品新品，质量与正式上架商品一致。\\n\\n4、众测商品买下后多久发货？\\n\\n您好，请以商品页显示为准。\\n\\n5、提交活动订单后多久内支付？\\n\\n您好，提交订单后最长付款时效为24小时，逾期订单自动取消\\n\\n6、成功下单后怎么查询众测订单？\\n\\n您好，成功下单后，您可以通过华为商城手机APP、华为商城手机WAP版、华为商城电脑网页版任意一端登录下单账户，在“我的订单”查询。（众测商品订单查询方式等同于正常商品订单）\\n\\n7、订单支付后未发货前可以取消订单吗？\\n\\n您好，在发货前消费者可以取消订单。\\n\\n8、取消订单后多久内退回款项？\\n\\n您好，和华为商城智能家居生态产品通用退款时效一致，3\\n\\n5个工作日。\\n\\n9、一个账号可以参与几次众测活动？有限购吗？\\n\\n您好，一个账号参与众测活动的次数无限制，没有限购。但每次订单只能购买一个商品。如您需要购买多个产品，可以多次参与众测活动。'),\n",
       " Document(metadata={'source': '/home/zpc/langchain/blibli/data/yjhx.md'}, page_content='一、选择回收物品\\n\\n点击VMALL您可以在设备回收分类中选择旧商品的类别以及品牌，现在VMALL支持回收部分品牌和型号的手机、笔记本、平板、智能数码以及摄影摄像。如若在此页面上找不到您的品牌或型号，您可以点击品牌栏右下方的加号（如下图红圈部分所示）查看VMALL所支持回收的所有品牌型号。首页滚动电子横幅右下方“以旧换新”标志（如下图红框部分所示），进入“以旧换新”页面。\\n\\n以旧换新图片\\n\\n（以下图片为参考流程，实际以页面为准）\\n\\n设备回收图片\\n\\n选择好您的旧商品后请点击商品图标，进入价格评估页面。\\n\\n二、价格评估\\n\\n在价格评估界面上，您需要对旧商品的基本信息（网络制式、存储容量、购买渠道、机身颜色），外观成色（边框背板，屏幕外观，屏幕显示，维修情况），和多功能选项参数进行评估，请如实填写您的手机情况。完成所有参数选择，请估价按钮（如下图红框部分所示）。\\n\\n完成所有参数选择，请点击“免费询价”按钮（如下图红框部分所示）。\\n\\n免费询价图片\\n\\n三、价格评估结果\\n\\n您可以查看手机的估价和这款手机的回收价格趋势，如果填写有什么问题，可以点击“重新询价”或“重新选择机型”更改；确认无误后，点击“换钱”按钮进入订单页面。\\n\\n价格评估结果图片\\n\\n四、提交订单\\n\\n在订单页面，您可以从以下两种常规回收方式选择一种进行回收，并提交订单结账。（回收宝支持顺丰上门、上门质检回收，爱回收支持顺丰上门、质检上门、信用回收）\\n\\n图5.png\\n\\n（1）顺丰上门回收操作流程\\n\\n① 回收方式：选择顺丰快递回收\\n\\n② 提交订单：填写联系人、电话号码、取货地址及上门时间，提交订单\\n\\n③ 上门取货：回收订单提交成功，快递员会按照填写的上门时间上门取货\\n\\n④ 确认交易：回收商会在收到旧机的48小时内完成检测，若价格与您提交订单时的价格一致，将立即发放华为代金券。\\n\\n（2）质检师上门回收操作流程\\n\\n① 回收方式：选择质检师上门回收。\\n\\n② 提交订单：填写联系人信息、手机号码、上门地址及上门时间，提交订单\\n\\n③ 确认交易：如通过质检师上门回收，在完成检测后，代金券可实时到账。\\n\\n质检师上门回收操作流程图片\\n\\n五、以旧换新答疑 ：更多关于以旧换新的问题请点击 “以旧换新答疑”查看。'),\n",
       " Document(metadata={'source': '/home/zpc/langchain/blibli/data/faq_1.txt'}, page_content='一、什么是0分期利息\\n\\n您好，“0分期利息”是指买家使用花呗、招行掌上生活、工行信用卡、银联信用卡等其他分期购物时无需支付分期利息的功能，分期利息由华为商城承担。\\n\\n注：自2023年起，商城将相关宣传将“免息”调整为“0分期利息”，主要基于中国银保监会、中国人民银行《关于进一步促进信用卡业务规范健康发展的通知》（银保监规〔2022〕13号），要求“银行业金融机构应当在分期业务合同（协议）首页和业务办理页面以明显方式展示分期业务可能产生的所有息费项目、年化利率水平和息费计算方式。向客户展示分期业务收取的资金使用成本时，应当统一采用利息形式，并明确相应的计息规则，不得采用手续费等形式，法律法规另有规定的除外。”\\n\\n二、可以参与0分期利息活动的商品\\n\\n商城目前仅支持部分单品参与0分期利息，若多商品（含不支持0分期利息）合并支付则不支持0分期利息，以支付页面为准，后续会逐渐开放更多商品，敬请关注。\\n\\n三、确认订单分期成功\\n\\n订单提交成功后在支付方式页面选择分期支付，点选显示0分期利息的支付方式及具体0分期利息期数后，完成支付。\\n\\n四、订单中有多个商品，其中有商品支持0分期利息，为什么提交后却没有0分期利息？\\n\\n0分期利息商品不能和其它商品一起购买，如果和其他商品购买而不能享用0分期利息，建议取消原来的订单，重新购买时把0分期利息商品和其他商品分开单独购买；且0分期利息的分期数是固定的，如6期0分期利息，并不是3/6/12都提供0分期利息的。\\n\\n五、小程序是否支持0分期利息？\\n\\n华为商城小程序暂不支持0分期利息。')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"/home/zpc/langchain/blibli/data\", show_progress=True)\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f4a14d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7fa9a90edba0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorStoreDB = FAISS.from_documents(docs, embedding=embeddings)\n",
    "vectorStoreDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f50df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_144345/3465606208.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  information = retriver.get_relevant_documents(\"请说说回收手机怎么办\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='f0694e70-ff8f-462a-99bb-f4e144eff351', metadata={'source': '/home/zpc/langchain/blibli/data/yjhx.md'}, page_content='一、选择回收物品\\n\\n点击VMALL您可以在设备回收分类中选择旧商品的类别以及品牌，现在VMALL支持回收部分品牌和型号的手机、笔记本、平板、智能数码以及摄影摄像。如若在此页面上找不到您的品牌或型号，您可以点击品牌栏右下方的加号（如下图红圈部分所示）查看VMALL所支持回收的所有品牌型号。首页滚动电子横幅右下方“以旧换新”标志（如下图红框部分所示），进入“以旧换新”页面。\\n\\n以旧换新图片\\n\\n（以下图片为参考流程，实际以页面为准）\\n\\n设备回收图片\\n\\n选择好您的旧商品后请点击商品图标，进入价格评估页面。\\n\\n二、价格评估\\n\\n在价格评估界面上，您需要对旧商品的基本信息（网络制式、存储容量、购买渠道、机身颜色），外观成色（边框背板，屏幕外观，屏幕显示，维修情况），和多功能选项参数进行评估，请如实填写您的手机情况。完成所有参数选择，请估价按钮（如下图红框部分所示）。\\n\\n完成所有参数选择，请点击“免费询价”按钮（如下图红框部分所示）。\\n\\n免费询价图片\\n\\n三、价格评估结果\\n\\n您可以查看手机的估价和这款手机的回收价格趋势，如果填写有什么问题，可以点击“重新询价”或“重新选择机型”更改；确认无误后，点击“换钱”按钮进入订单页面。\\n\\n价格评估结果图片\\n\\n四、提交订单\\n\\n在订单页面，您可以从以下两种常规回收方式选择一种进行回收，并提交订单结账。（回收宝支持顺丰上门、上门质检回收，爱回收支持顺丰上门、质检上门、信用回收）\\n\\n图5.png\\n\\n（1）顺丰上门回收操作流程\\n\\n① 回收方式：选择顺丰快递回收\\n\\n② 提交订单：填写联系人、电话号码、取货地址及上门时间，提交订单\\n\\n③ 上门取货：回收订单提交成功，快递员会按照填写的上门时间上门取货\\n\\n④ 确认交易：回收商会在收到旧机的48小时内完成检测，若价格与您提交订单时的价格一致，将立即发放华为代金券。\\n\\n（2）质检师上门回收操作流程\\n\\n① 回收方式：选择质检师上门回收。\\n\\n② 提交订单：填写联系人信息、手机号码、上门地址及上门时间，提交订单\\n\\n③ 确认交易：如通过质检师上门回收，在完成检测后，代金券可实时到账。\\n\\n质检师上门回收操作流程图片\\n\\n五、以旧换新答疑 ：更多关于以旧换新的问题请点击 “以旧换新答疑”查看。'),\n",
       " Document(id='c194f283-5bfe-4b71-9077-dcfdd4a9f68c', metadata={'source': '/home/zpc/langchain/blibli/data/faq_1.txt'}, page_content='一、什么是0分期利息\\n\\n您好，“0分期利息”是指买家使用花呗、招行掌上生活、工行信用卡、银联信用卡等其他分期购物时无需支付分期利息的功能，分期利息由华为商城承担。\\n\\n注：自2023年起，商城将相关宣传将“免息”调整为“0分期利息”，主要基于中国银保监会、中国人民银行《关于进一步促进信用卡业务规范健康发展的通知》（银保监规〔2022〕13号），要求“银行业金融机构应当在分期业务合同（协议）首页和业务办理页面以明显方式展示分期业务可能产生的所有息费项目、年化利率水平和息费计算方式。向客户展示分期业务收取的资金使用成本时，应当统一采用利息形式，并明确相应的计息规则，不得采用手续费等形式，法律法规另有规定的除外。”\\n\\n二、可以参与0分期利息活动的商品\\n\\n商城目前仅支持部分单品参与0分期利息，若多商品（含不支持0分期利息）合并支付则不支持0分期利息，以支付页面为准，后续会逐渐开放更多商品，敬请关注。\\n\\n三、确认订单分期成功\\n\\n订单提交成功后在支付方式页面选择分期支付，点选显示0分期利息的支付方式及具体0分期利息期数后，完成支付。\\n\\n四、订单中有多个商品，其中有商品支持0分期利息，为什么提交后却没有0分期利息？\\n\\n0分期利息商品不能和其它商品一起购买，如果和其他商品购买而不能享用0分期利息，建议取消原来的订单，重新购买时把0分期利息商品和其他商品分开单独购买；且0分期利息的分期数是固定的，如6期0分期利息，并不是3/6/12都提供0分期利息的。\\n\\n五、小程序是否支持0分期利息？\\n\\n华为商城小程序暂不支持0分期利息。')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = vectorStoreDB.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "\n",
    "information = retriver.get_relevant_documents(\"请说说回收手机怎么办\")\n",
    "information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f3a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-08T01:57:28+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-08T01:57:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/home/zpc/langchain/blibli/data/pdf/pdf/Langchain┤є─г╨═AI╙ж╙├╩╡╒╜┐к╖в-╫╩┴╧/01-Langchain/pdf/2312.04005.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='KOALA: Self-Attention Matters in Knowledge Distillation of Latent\\nDiffusion Models for Memory-Efficient and Fast Image Synthesis\\nYoungwan Lee1,2 Kwanyong Park1 Yoorhim Cho3 Yong-Ju Lee1 Sung Ju Hwang2\\n1Electronics and Telecommunications Research Institute (ETRI), South Korea\\n2Korea Advanced Institute of Science and Technology (KAIST), South Korea\\n3Sookmyung Women’s University, South Korea\\nproject page: https://youngwanlee.github.io/KOALA/\\nAbstract\\nStable diffusion is the mainstay of the text-to-image (T2I)\\nsynthesis in the community due to its generation perfor-\\nmance and open-source nature. Recently, Stable Diffusion\\nXL (SDXL), the successor of stable diffusion, has received a\\nlot of attention due to its significant performance improve-\\nments with a higher resolution of 1024 × 1024 and a larger\\nmodel. However, its increased computation cost and model\\nsize require higher-end hardware (e.g., bigger VRAM GPU)\\nfor end-users, incurring higher costs of operation. To ad-\\ndress this problem, in this work, we propose an efficient la-\\ntent diffusion model for text-to-image synthesis obtained by\\ndistilling the knowledge of SDXL. To this end, we first per-\\nform an in-depth analysis of the denoising U-Net in SDXL,\\nwhich is the main bottleneck of the model, and then design\\na more efficient U-Net based on the analysis. Secondly, we\\nexplore how to effectively distill the generation capability\\nof SDXL into an efficient U-Net and eventually identify four\\nessential factors, the core of which is that self-attention is\\nthe most important part. With our efficient U-Net and self-\\nattention-based knowledge distillation strategy, we build\\nour efficient T2I models, called KOALA-1B &-700M, while\\nreducing the model size up to 54% and 69% of the origi-\\nnal SDXL model. In particular, the KOALA-700M is more\\nthan twice as fast as SDXL while still retaining a decent\\ngeneration quality. We hope that due to its balanced speed-\\nperformance tradeoff, our KOALA models can serve as a\\ncost-effective alternative to SDXL in resource-constrained\\nenvironments.\\n1. Introduction\\nSince the emergence of the Stable diffusion mod-\\nels (SDMs) [41, 47, 48] which are based on the latent dif-\\nfusion model [46], not only has text-to-image synthesis\\ngreatly advanced but also applications utilizing it have been\\nactively developed, such as image editing [8, 63], control-\\nlable image synthesis [38, 71] personalized image synthe-\\nsis [12, 30, 52], text-to-video generation [3, 25] and 3D as-\\nset synthesis [32, 42, 70]. While these downstream tasks\\nbenefit from SDM’s superior image generation quality as a\\nbackbone, its massive computation costs and large model\\nsize require expensive hardware equipment and thus incur\\nhuge costs. Furthermore, a more recent version of the sta-\\nble diffusion model, SDXL [41], demonstrates significantly\\nimproved image generation quality with a higher resolution\\nof 1024 × 1024, but at the cost of more computations and\\nmemory requirement.\\nTo alleviate this computation burden, several works\\nhave been proposed, which introduce quantization [59],\\nhardware-aware optimization [7, 9], denoising step reduc-\\ntion [31, 37, 54], and architectural model optimization [26,\\n31]. In particular, the denoising step reduction [31, 37, 54]\\nand architectural model compression [26] methods adopt\\nthe knowledge distillation (KD) scheme [15, 18] by allow-\\ning the model to mimic the output of the SDM as a teacher\\nmodel. The step-distillation methods [31, 37, 54] allow the\\ndenoised latent of the diffusion model in the early denois-\\ning steps to mimic the output in the later denoising steps of\\nthe teacher model. As an orthogonal work for the architec-\\ntural model compression, BK-SDM [26] exploits KD when\\ncompressing the most heavy-weight part, U-Net, in SDM-\\nv1.4 [47]. BK-SDM builds a compressed U-Net by simply\\nremoving some blocks and allows the compressed U-Net\\nto mimic the last features at each stage and the predicted\\nnoise of the teacher model during the pre-training phase.')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/home/zpc/langchain/blibli/data/pdf/pdf/Langchain┤є─г╨═AI╙ж╙├╩╡╒╜┐к╖в-╫╩┴╧/01-Langchain/pdf/2312.04005.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce9ac359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KOALA: Self-Attention Matters in Knowledge Distillation of Latent\\nDiffusion Models for Memory-Efficient and Fast Image Synthesis\\nYoungwan Lee1,2 Kwanyong Park1 Yoorhim Cho3 Yong-Ju Lee1 Sung Ju Hwang2\\n1Electronics and Telecommunications Research Institute (ETRI), South Korea\\n2Korea Advanced Institute of Science and Technology (KAIST), South Korea\\n3Sookmyung Women’s University, South Korea\\nproject page: https://youngwanlee.github.io/KOALA/\\nAbstract\\nStable diffusion is the mainstay of the text-to-image (T2I)\\nsynthesis in the community due to its generation perfor-\\nmance and open-source nature. Recently, Stable Diffusion\\nXL (SDXL), the successor of stable diffusion, has received a\\nlot of attention due to its significant performance improve-\\nments with a higher resolution of 1024 × 1024 and a larger\\nmodel. However, its increased computation cost and model\\nsize require higher-end hardware (e.g., bigger VRAM GPU)\\nfor end-users, incurring higher costs of operation. To ad-\\ndress this problem, in this work, we propose an efficient la-\\ntent diffusion model for text-to-image synthesis obtained by\\ndistilling the knowledge of SDXL. To this end, we first per-\\nform an in-depth analysis of the denoising U-Net in SDXL,\\nwhich is the main bottleneck of the model, and then design\\na more efficient U-Net based on the analysis. Secondly, we\\nexplore how to effectively distill the generation capability\\nof SDXL into an efficient U-Net and eventually identify four\\nessential factors, the core of which is that self-attention is\\nthe most important part. With our efficient U-Net and self-\\nattention-based knowledge distillation strategy, we build\\nour efficient T2I models, called KOALA-1B &-700M, while\\nreducing the model size up to 54% and 69% of the origi-\\nnal SDXL model. In particular, the KOALA-700M is more\\nthan twice as fast as SDXL while still retaining a decent\\ngeneration quality. We hope that due to its balanced speed-\\nperformance tradeoff, our KOALA models can serve as a\\ncost-effective alternative to SDXL in resource-constrained\\nenvironments.\\n1. Introduction\\nSince the emergence of the Stable diffusion mod-\\nels (SDMs) [41, 47, 48] which are based on the latent dif-\\nfusion model [46], not only has text-to-image synthesis\\ngreatly advanced but also applications utilizing it have been\\nactively developed, such as image editing [8, 63], control-\\nlable image synthesis [38, 71] personalized image synthe-\\nsis [12, 30, 52], text-to-video generation [3, 25] and 3D as-\\nset synthesis [32, 42, 70]. While these downstream tasks\\nbenefit from SDM’s superior image generation quality as a\\nbackbone, its massive computation costs and large model\\nsize require expensive hardware equipment and thus incur\\nhuge costs. Furthermore, a more recent version of the sta-\\nble diffusion model, SDXL [41], demonstrates significantly\\nimproved image generation quality with a higher resolution\\nof 1024 × 1024, but at the cost of more computations and\\nmemory requirement.\\nTo alleviate this computation burden, several works\\nhave been proposed, which introduce quantization [59],\\nhardware-aware optimization [7, 9], denoising step reduc-\\ntion [31, 37, 54], and architectural model optimization [26,\\n31]. In particular, the denoising step reduction [31, 37, 54]\\nand architectural model compression [26] methods adopt\\nthe knowledge distillation (KD) scheme [15, 18] by allow-\\ning the model to mimic the output of the SDM as a teacher\\nmodel. The step-distillation methods [31, 37, 54] allow the\\ndenoised latent of the diffusion model in the early denois-\\ning steps to mimic the output in the later denoising steps of\\nthe teacher model. As an orthogonal work for the architec-\\ntural model compression, BK-SDM [26] exploits KD when\\ncompressing the most heavy-weight part, U-Net, in SDM-\\nv1.4 [47]. BK-SDM builds a compressed U-Net by simply\\nremoving some blocks and allows the compressed U-Net\\nto mimic the last features at each stage and the predicted\\nnoise of the teacher model during the pre-training phase.removing some blocks and allows the compressed U-Net\\nto mimic the last features at each stage and the predicted\\nnoise of the teacher model during the pre-training phase.\\nHowever, the compression method proposed by BK-SDM\\nachieves a limited compression rate (33%) when applied to\\nthe larger SDXL than SDM-v1.4 and the strategy for feature\\n1\\narXiv:2312.04005v1  [cs.CV]  7 Dec 2023Portraitphotoofastandinggirl,photograph,goldenhair,depthoffield,moodylight,goldenhour,centered,extremel-ydetailed,awardwinningphotography,realisticProfessional portrait photo of an anthropomorphic cat wearing fancy gentleman hat and jacket walking in autumn forest. Cute toy owl made of suede, geometric accurate, relief on skin, plastic relief surface of body, intricate details, cinematic\\nAlbertEinsteininasurrealistCyberpunk2077world,hyper-realistic Pirateshiptrappedinacosmicmaelstromnebula,renderedincosmicbeachwhirlpoolengine,volumetriclighting,spect-acular,ambientlights,lightpollution,cinematicatmosphere,artnouveaustyle,illustrationartartwork,intricatedetail.\\nCutesmalldogsittinginamovietheaterwatchingamovie,unrealengine,cozyindoorlighting,artstation,detailed,digit-alpainting,cinematic,characterdesignbypixar,hyperrealis-tic,octanerender\\nFigure 1. Generated samples by our KOALA-700M trained by the proposed knowledge-distillation approach with SDXL [41]. With\\nthe following settings: FP-16 precision, 1024 × 1024 resolution, and 25 denoising steps with Euler discrete scheduler [24] same as the\\nhuggingface’s SDXL-Base-1.0 model [66], the inference time is 1.4 seconds on an NVIDIA 4090 (24GB) GPU, which is over 2× faster\\nthan SDXL-Base-1.0 (3.3s) while reducing the U-Net model size by 69%.\\ndistillation in U-Net has not yet been fully explored.\\nIn this work, our goal is to build a more efficient text-\\nto-image synthesis model by distilling the generation ca-\\npability of SDXL [41]. To this end, we first perform an\\nin-depth analysis of SDXL’s denoising U-Net, which re-\\nquires the most number of parameters and computational\\ncost, and find that most of the parameters are concentrated\\nat the lowest feature level due to the large number of trans-\\nformer blocks. Based on the analysis, we design an effi-\\ncient U-Net by reducing the origin SDXL’s U-Net by up\\nto 69% (vs. BK’s method: 33%). Furthermore, we inves-\\ntigate how to effectively distill SDXL as a teacher model\\nand find four essential factors for feature-level knowledge\\ndistillation. The core of these findings is that self-attention\\nfeatures are the most crucial for distillation due to the fact\\nthat self-attention-based KD allows models to learn more\\ndiscriminative representations between objects or attributes.\\nWith our knowledge distillation (KD) strategies, we\\ntrain an efficient text-to-image synthesis model on top of\\nSDXL [41], called KOALA, by only replacing SDXL’s\\nU-Net with our efficient U-Net. KOALA is trained on a\\nsmaller publicly available LAION-Aesthestics-V2-6+ [57],\\nwhich has only 8M text-image pairs. Recent studies [2,\\n28, 67, 68] have shown that FID [17] is not well corre-\\nlated with the fidelity of the generated image, and thus\\nwe use two alternative evaluation metrics: Human Pref-\\nerence Score (HPSv2) [67] for visual aesthetics and T2I-\\nCompbench [21] for image-text alignment. Our efficient\\nKOALA models consistently outperform BK-SDM [26]’s\\nKD method in both metrics. Furthermore, our smaller\\n2model, KOALA-700M, shows better performance than\\nSDM-v2.0 [48], which is one of the most widely used in\\nthe community, while having a similar model size and in-\\nference speed. Lastly, to validate its practical impact, we\\nperform inference analysis on a variety of consumer-grade\\nGPUs with different memory sizes ( e.g., 8GB, 11GB, and\\n24GB), and the results show that whereas SDXL cannot be\\nmounted on an 8GB GPU, our KOALA-700M can run on\\nit while still retaining decent image generation quality as\\nshown in Fig. 1. Our main contributions are as follows:\\n1. We design two efficient denoising U-Net architectures\\nwith model sizes (1.13B/782M) more than twice as small\\nas SDXL’s U-Net (2.56B).\\n2. We perform a comprehensive analysis of the knowledge\\ndistillation strategies for SDXL, finding four essential\\nfactors for feature distillation.\\n3. We build two efficient T2I models pre-trained by the\\nproposed KD, called KOALA-1B/700M, which is more\\nthan 2× smaller and faster compared to SDXL-Base.\\n4. We perform a systemical analysis of inference on a vari-\\nety of GPUs, showing that our KOALA-700M can oper-\\nate on an economical GPU with 8 GB of memory.\\n2. Related Works\\nKnowledge distillation for efficient T2I diffusion mod-\\nels. Denoising diffusion models [20, 61] dominate the\\nrecent state-of-the-art text-to-image (T2I) diffusion mod-\\nels [10, 44, 46, 53] due to their unprecedented high qual-\\nity and diversity. However, a significant shortcoming of\\nthese models is their intensive computational demands dur-\\ning sampling time, which constrains their utility in practical\\nscenarios. To alleviate this problem, early efforts [37, 54]\\nhave focused on improving the sampling speed by reduc-\\ning the number of required sampling steps. In particular,\\nSalimans et al. [54] proposes a concept of step distillation,\\nwhich trains a student model with fewer steps, distilled from\\na pre-trained diffusion model as a teacher model. Meng\\net al. [37] expand this concept to classifier-free guided dif-\\nfusion models, facilitating the step distillation for modern\\ntext-to-image diffusion models. Although these methods\\nsignificantly speed up the inference of models, the hardware\\nprerequisites still pose challenges to practitioners.\\nAs another line of research, BK-SDM [26] attempts\\nan architectural compression of diffusion models. They\\nfirst eliminate redundant network components to construct\\na shallow model. Then, a simple knowledge distilla-\\ntion method [15, 50] is employed to transfer the knowl-\\nedge from the original pre-trained diffusion model ( e.g.,\\nSDM-v1.4 [47]). Remarkably, this compressed model has\\nachieved substantial reductions in sampling time, GPU\\nmemory demands, and storage requirements, with only\\na modest degradation in performance. However, BK-\\nSDM’s simple block removal method has limitations in\\nDW1_RDW2_RDW2_TDW3_RDW3_TMID_RMID_TUP1_RUP1_TUP2_RUP2_TUP3_R\\nStage\\n0\\n200\\n400\\n600\\n800\\n1000Parameter (M)\\nSDXL (2.57B)\\nKOALA-1B (1.16B)\\nKOALA-700M (0.78B)\\nFigure 2. Dissection of U-Net in SDXL. DWi and UPi indicate\\ni-th stage of the down and the up block, and R and T denote the\\nResidual block and Transformer block, respectively.\\nSDXL-Base Text Encoder [22, 43] V AE Decoder [4] U-Net\\n#Parameters 817M 83M 2,567M\\nLatency (s) 0.008 0.002 3.133\\nTable 1. SDXL-Base-1.0 model budget. Latency is measured\\nunder the image scale of 1024 × 1024, FP16-precision, and 25\\ndenoising steps in NVIDIA 4090 GPU (24GB).\\nU-Net SDM-v2.0 SDXL-BaseKOALA-1B KOALA-700M\\nParam. 865M 2,567M 1,161M 782M\\nCKPT size 3.46GB 10.3GB 4.4GB 3.0GB\\nTx blocks [1, 1, 1, 1] [0, 2, 10] [0, 2, 6] [0, 2, 5]\\nMid block ✓ ✓ ✓ ✗\\nLatency 1.131s 3.133s 1.604s 1.257s\\nTable 2. U-Net Comparison. Tx means Transformer. SDM-\\nv2.0 [48] uses 768 × 768 resolution, while SDXL and KOALA\\nmodels use 1024 × 1024 resolution. Latency is measured\\nwith FP16-precision, and 25 denoising steps in NVIDIA 4090\\nGPU (24GB). CKPT means the trained checkpoint file.\\ncompressing more complex and larger U-Net models suchwith FP16-precision, and 25 denoising steps in NVIDIA 4090\\nGPU (24GB). CKPT means the trained checkpoint file.\\ncompressing more complex and larger U-Net models such\\nas SDXL [41]. Beyond the BK-SDM method, which only\\ndistills the last feature at each stage, there is still room for\\nfurther exploration in distilling knowledge from more com-\\nplex U-Net in SDXL.\\n3. Analysis: Stable Diffusion XL\\nSDXL [41], the latest version of the SDM series [46–48],\\nexerts a significant influence on both the academic com-\\nmunity and the industry due to its unprecedented quality\\nand open source resources. It has several key improve-\\nment points from the previous SDM-v2.0 [48], e.g., multi-\\nple sizes- & crop-conditioning, improved V AE [27, 45], and\\nmuch larger U-Net [51], and an ad hoc style of refinement\\nmodule, which leads to significantly improving generation\\n3CR1,1R1,2CDwR2,1Ts2,1R2,2Ts2,2CDwR3,1Ts3,1R3,2Ts3,2R4,1Ts4,1R4,2 CR6,1Ts6,1R6,2Ts6,2R6,3Ts6,3CUpR7,1R7,2R7,3TeacherSDXL-Base(2.56B)\\nStudentKOALA-1B(1.16B)\\nKOALA-700M(782M)\\n320×64×64\\nCR1,1CDwR2,1Ts2,1 CDwR3,1Ts3,1 R4,1Ts4,1R4,2\\nR5,1Ts5,1R5,2Ts5,2R5,3Ts5,3CUp\\nC\\nDW-1 DW-2 DW-3 MidUP-1 UP-2 UP-3\\nDW-1 DW-2 DW-3 MidR5,1Ts5,1 R5,2Ts5,2CUpR6,1Ts6,1 R6,2Ts6,2CUpUP-2 R7,1R7,2UP-3\\nCR1,1CDwR2,1Ts2,1 CDwR3,1Ts3,1 CDW-1 DW-2 DW-3 R5,1Ts5,1 R5,2Ts5,2CUpR6,1Ts6,1 R6,2Ts6,2CUpUP-2 R7,1R7,2\\n640×32×32 1280×32×321280×32×32 1280×64×64 640×128×128320×128×128320×128×128\\nUP-1\\nUP-1 UP-3\\nR ResBlockTsTransformerBlocks= =Depth = 2TsTransformerBlocks=Depth = 6TsTransformerBlocks=Depth = 10TsTransformerBlocks=Depth = 5\\nCCDwCUpConvolution Block\\nlatent!\\nlatent!\\nPredictednoise\\nPredictednoise\\n4×128×128\\n4×128×128\\n4×128×128\\n4×128×128\\nSelf-attention-basedknowledge distillation(Sec. 4.2)\\nFigure 3. Overview of KnOwledge-DistillAtion in LAtent diffusion model based on SDXL and architecture of KOALA. We omit\\nskip connections for simplicity. We perform feature distillation in transformer blocks using the output of the self-attention layer.\\nquality. However, the significant enlargement of U-Net in\\nmodel size results in increased computational costs and sig-\\nnificant memory (or storage) requirements, hampering the\\naccessibility of SDXL. Thus, we investigate the U-Net in\\nSDXL regarding model size and latency to design a more\\nlightweight U-Net for knowledge distillation. We dissect\\nthe components of SDXL, quantifying its size and latency\\nduring the denoising phase, as detailed in Tab. 1. The en-\\nlarged U-Net (2.56B) is the primary cause of the increasing\\nSDXL model size (vs. SDM-v2.0 (865 M)). Furthermore,\\nthe latency of U-Net is the main inference time bottleneck in\\nSDXL. Therefore, it is necessary to reduce U-Net’s model\\nbudget for better efficiency.\\nThe SDXL’s U-Net architecture varies in the number of\\ntransformer blocks for each stage, unlike SDM-v2.0, which\\nemploys a transformer block for each stage (see Tab. 2). At\\nthe highest feature levels ( e.g., DW-1&UP-3 in Fig. 3),\\nSDXL uses only residual blocks [14] without transformer\\nblocks, instead distributing more transformer blocks to\\nlower-level features. So, in Fig. 2, we analyze the parame-\\nter distribution of each stage in the U-Net. Most parameters\\nare concentrated on the transformers with ten blocks in the\\nlowest feature map ( e.g., 32 × 32 of DW-3, Mid, UP-1\\nin Fig. 3), making the main parameter bottleneck. Thus,\\nit is essential to address this bottleneck when designing an\\nefficient U-Net.\\n4. Approach\\nIn this section, we first propose a simple yet efficient U-Net\\narchitecture in Sec. 4.1. Then, we explore how to effectively\\ndistill the knowledge from U-Net in SDXL [41] into the\\nproposed efficient U-Net in Sec. 4.2.\\n4.1. Efficient U-Net architecture\\nBased on the investigation of the U-Net model budget of\\nSDXL in Sec. 3, we propose a simple yet efficient U-Net\\narchitecture. BK-SDM [26] also aimed to compress the\\nU-Net of SDM-v1.4 [47] by removing a pair of a residual\\nblock and a transformer block at each stage. However, the\\nBK-SDM’s approach is only suitable for SDM-v1.4, which\\nhas only one transformer block (i.e., depth=1) at each stage.\\nAs SDXL’s U-Net has a different number of transformer\\nblocks at each stage, as shown in Tab. 2, simple block-level\\nremoval (one block pair) can only reduce SDXL’s U-Net to\\nat most 1.3B model parameters.\\nIn this work, we devise a compressed U-Net that is\\nmore suitable for SDXL [41] compared to that of BK-\\nSDM [26]. Similar to BK-SDM, we first remove the\\nresidual-transformer blocks pair at each stage. Specifically,\\nin the encoder part ( DW-i), each stage has two alternat-\\ning pairs of a residual block and transformer blocks. We\\nremove the last pair of residual-transformer blocks at each\\nstage. In the decoder part ( UP-i), we remove the inter-\\nmediate pair of residual-transformer blocks. Furthermore,\\nfocusing on the fact that the majority of the parameters arestage. In the decoder part ( UP-i), we remove the inter-\\nmediate pair of residual-transformer blocks. Furthermore,\\nfocusing on the fact that the majority of the parameters are\\nconcentrated on the transformer blocks at the lowest fea-\\ntures (Fig. 2), we reduce the depth of the transformer blocks\\nfrom 10 to 5 or 6 at the lowest features ( i.e., DW-3, Mid\\nand UP-1 in Fig. 3). As a result, we design two types of\\ncompressed U-Net, KOALA-1B and KOALA-700M. More\\ndetails of the proposed U-Nets are demonstrated in Tab. 2\\nand Fig. 3. Note that we remove Mid block in KOALA-\\n700M for additional model compression. Our KOALA-1B\\nmodel has 1.16B parameters, making it twice as compact as\\nSDXL (2.56B). Meanwhile, KOALA-700M, with its 782M\\nparameters, is comparable in size to SDM-v2.0 (865M). It\\nis noteworthy that KOALA-700M achieves almost twice the\\n4Distill type HPSv2\\nSD-loss 25.53\\nSA 26.74\\nCA 26.11\\nRes 26.27\\nFFN 26.48\\nLF (BK [26]) 26.63\\n(a) Distillation type.\\nDistill loc. HPSv2\\nSD-loss 25.53\\nDW-2 25.32\\nDW-3 25.57\\nMid 25.66\\nUP-1 26.52\\nUP-2 26.05\\n(b) Distill stage. We distill the SA\\nfeature at only each stage.\\nSA loc. HPSv2\\nSA-bottom 26.74\\nSA-interleave 26.58\\nSA-up 26.48\\n(c) SA locations to distill from a\\ntransformer block with a depth of\\n10 in Teacher U-Net in SDXL.\\nCombination HPSv2\\nBaseline (SA only) 26.74\\nSA + LF at DW-1 & UP-3 26.98\\nSA + Res at DW-1 & UP-3 26.94\\nSA + LF all 26.83\\nSA + Res all 26.80\\nSA+CA+Res+FFN+LF all 26.39\\n(d) Combination. DW-1 & UP-3 are the\\nhighest feature resolution in U-Net.\\nTable 3. Analysis of feature level knowledge distillation of U-Net in SDXL [41].SA, CA, and FFN denote self-attention, cross-attention,\\nand feed-forward net in the transformer block. Res is a convolutional residual block and LF denotes the last feature (same in BK [26]). For\\nthe ablation study, we train our KOALA-1B as student U-Net for 30K iterations with a batch size of 32.\\nfaster inference speed over SDXL, as well as comparable to\\nSDM-v2.0, which generates lower-resolution images.\\n4.2. Exploring Knowledge distillation for SDXL\\nNow we explore how to effectively distill the knowledge\\nof U-Net in SDXL [41] into the proposed compact U-Net\\ndescribed in Sec. 4.1. As a latent diffusion model [46],\\nSDXL encodes input x ∈ R3×1024×1024 to latent represen-\\ntation z ∈ R4×128×128 via V AE [27, 45] to reduce compu-\\ntation cost for high-resolution generation. The latent rep-\\nresentation z is then fed into the U-Net [51] to predict the\\nnoise (ϵ), which is the most essential part of generating the\\nimage and also the most computationally intensive. We re-\\nplace this U-Net in SDXL with the proposed efficient U-\\nNet and then train the replaced U-Net through knowledge-\\ndistillation-based learning.\\nKim et al. [26] adopted knowledge distillation for text-\\nto-image pre-training by using the U-Net in SDM-v1.4 [47]\\nas a teacher model ( Tθ) through the following objectives\\nLtask + LoutKD + LfeatKD:\\nLtask = min\\nSθ\\nEzt,ϵ,t,c||ϵt − ϵSθ (zt, t, c)||2\\n2, (1)\\nLoutKD = min\\nSθ\\nEz,ϵ,t,c||ϵTθ (z, t, c) − ϵSθ (z, t, c)||2\\n2, (2)\\nLfeatKD = min\\nSθ\\nEz,ϵ,c,t||\\nX\\ni\\nfi\\nT (zt, t, c) − fi\\nS(zt, t, c)||2\\n2,\\n(3)\\nwhere Sθ is the compressed U-Net as a student model, ϵt\\nis the ground-truth sampled Gaussian noise at timestep t, c\\nis text embedding as a condition, ϵSθ (·) and ϵTθ (·) denote\\nthe predicted noise from each U-Net in teacher and student\\nmodel, respectively. Ltask is the task loss for the reverse de-\\nnoising process [20], Lout is the output-KD loss [18] com-\\nputed between the predicted noises from teacher and student\\nrespectively, and Lfeat is the feature-wise KD loss [15, 50]\\ncomputed between the last features fi\\nT (·) and fi\\nT (·) at i-\\nstage from teacher and student models, respectively.\\nThe feature-wise distillation literature [6, 13, 15, 39, 50]\\nshows that intermediate features play a more critical role\\nin knowledge distillation than output KD [18]. For the\\nfeature-wise KD-loss, BK-SDM [26] considers only the\\nlast feature map at each stage. However, the denois-\\ning U-Net consists of several types of features, such as\\nself-attention ( SA), cross-attention ( CA), and feedforward\\nnet (FFN) in the transformer block, and convolutional resid-\\nual block ( Res). This means that the feature distillation\\napproach for text-to-image diffusion models has not been\\nsufficiently explored, leaving room for further investigation.\\nIn this work, we have performed an in-depth analysis of\\nfeature distillation in the U-Net of SDXL [41] as shown\\nin Tab. 3 and observed four important findings . To this\\nend, we ablate feature distillation strategies by using our ef-\\nficient U-Net (KOALA-1B) as the student model and the\\nU-Net of SDXL as the teacher model. More training details\\nare described in Sec. 5.1. We start from a baseline trained\\nonly by Ltask and add LfeatKD without LoutKD to validate the\\neffect of feature distillation.are described in Sec. 5.1. We start from a baseline trained\\nonly by Ltask and add LfeatKD without LoutKD to validate the\\neffect of feature distillation.\\nF1. Which feature type is effective for distillation? BK-\\nSDM [26] demonstrated that distilling the last features (LF)\\nat U-Net stages benefits overall performance, when applied\\nto shallow U-Net of early SDM-v1.4 [47]. However, with\\nthe increasing complexity of U-Net and its stage, relying\\nsolely on LF may not be sufficient to mimic the intricate\\nbehavior of the teacher U-Net. To this end, we revisit\\nwhich features provide the richest guidance for effective\\nknowledge distillation. We focus on key intermediate fea-\\ntures from each stage: outputs from the SA, CA, and FFN\\nlayers in the transformer block, as well as outputs from\\nRes and LF. Tab. 3a summarizes the experimental results.\\nWhile all types of features help obtain higher performance\\nover the na ¨ıve baseline with only the task loss, distilling\\nself-attention features achieves the most performance gain.\\nConsidering the prior studies [29, 60, 62] which suggest that\\nSA plays a vital role in capturing semantic affinities and the\\noverall structure of images, the results emphasize that such\\ninformation is crucial for the distillation process.\\n5LF (BK)SA (Ours)\\n(a) Generated Image (b) PCA Analysis\\n (c) Self-attention visualization\\nDW2 DW3 MID UP1 UP2\\n“a big hippopotamus and a small rabbit.” \\nFigure 4. Analysis on self-attention maps of distilled student U-Nets. (a) Generated images of LF- and SA-based distilled models,\\nwhich are BK-SDM [26] and our proposal, respectively. In BK-SDM’s result, a rabbit is depicted like a hippopotamus ( i.e., appearance\\nleakage). (b) Visualization of PCA analysis results on self-attention maps of UP-1 stage. (c) Representative visualization of self-attention\\nmap from different U-Net stages. Red boxes denote the query patches. Note that from the MID stage, the SA-based model attends to the\\nrabbit more discriminatively than the LF model, demonstrating that self-attention-based KD allows to generate objects more distinctly.\\nTo understand the effects more clearly, we illustrate a\\nrepresentative example in the Fig. 4. To reason about\\nhow the distilled student U-Net captures self-similarity, we\\nperform a PCA analysis [23, 63] on self-attention maps.\\nSpecifically, we apply PCA on self-attention maps fromSA-\\nand LF-based models and show the top three principal com-\\nponents in Fig. 4-(b). Interestingly, in the SA-based model,\\neach principal component distinctly represents individual\\nobjects (i.e., unique color assignments to each object). This\\nindicates that the SA-based model effectively distinguishes\\ndifferent objects in modeling self-similarity, which plays a\\ncrucial role in accurately rendering the distinct appearance\\nof each object. In contrast, theLF-based model exhibits less\\ndistinction between objects, resulting in appearance leak-\\nage between them (e.g., small hippo with rabbit ears).\\nF2. Which stage is most effective for distillation? Based\\non the first finding (F1), we further explore the role and\\nsignificance of each self-attention stage. To this end, we\\nfirst visualize the self-attention map in Fig. 4-(c). The\\nself-attention maps initially capture general contextual in-\\nformation (e.g., DW-2&DW-3) and gradually focus on lo-\\ncalized semantics (e.g., MID). In the decoder, self-attentions\\nincreasingly correlate with higher-level semantics (e.g., ob-\\nject) to accurately model appearances and structures. No-\\ntably, in this stage, theSA-based model attends correspond-\\ning object regions (given the query patch, red box) more\\ndiscriminatively than the LF-based model, which results in\\nimproved compositional image generation performance.\\nIn addition, we ablate the significance of each self-\\nattention stage in the distillation process. Specifically, we\\nadopt an SA-based loss at a single stage alongside the task\\nloss. As shown in Tab. 3b, the results align with the above\\nunderstanding: distilling self-attention knowledge within\\nthe decoder stages significantly enhances generation qual-\\nity. In comparison, the impact of self-attention solely within\\nthe encoder stages is less pronounced. Consequently, we\\nopt to retain more SA layers within the decoder (see Fig. 3).\\nF3. Which SA’s location is effective in the trans-\\nformer blocks? At the lowest feature level, the depth of\\nthe transformer blocks is 6 for KOALA-1B, so we need\\nto decide which locations to distill from the 10 trans-\\nformer blocks of teacher U-Net. We assume three cases\\nfor each series of transformer blocks; (1) SA-bottom:\\n{fl\\nT | l ∈ {1, 2, 3, 4, 5}}, (2) SA-interleave: {fl\\nT |\\nl ∈ {1, 3, 5, 7, 9, 10}}, and (3) SA-up: {fl\\nT | l ∈\\n{6, 7, 8, 9, 10}} where l is the number of block. Tab. 3c\\nshows that SA-bottom performs the best while SA-up\\nperforms the worst. This result suggests that the features of\\nthe early blocks are more significant for distillation. A more\\nempirical analysis is described in Appendix B.2. Therefore,\\nwe adopt the SA-bottom strategy in all experiments.\\nF4. Which combination is the best? In SDXL’s U-Net,\\nas shown in Fig. 3, there are no transformer blocks at the\\nhighest feature levels ( e.g., DW-1&UP-3); consequently,F4. Which combination is the best? In SDXL’s U-Net,\\nas shown in Fig. 3, there are no transformer blocks at the\\nhighest feature levels ( e.g., DW-1&UP-3); consequently,\\nself-attention features cannot be distilled at this stage.\\nThus, we try two options: the residual block ( Res at\\nDW-1&UP-3) and the last feature ( LF at DW-1&UP-3)\\nas BK-SDM [26]. To this end, we perform SA-based fea-\\nture distillation at every stage except for DW-1 and UP-3,\\nwhere we use the above two options, respectively. In\\naddition, we try additional combinations: SA+LF all,\\nSA+Res all, and SA+CA+Res+FFN+LF all where\\nall means all stages). Tab. 3d demonstrates that adding\\nmore feature distillations to the SA-absent stage ( e.g.,\\n6Model #Param. HPSv2 Attribute Object Relationship Complex AverageWhole/U-Net Anime Concept-art Paintings Photo AverageColor Shape Texture Spatial Non-spatial\\nSDM-v1.4 [47] 1.04B 860M 27.26 26.61 26.66 27.27 26.95 0.3765 0.3576 0.4156 0.1246 0.3079 0.308 0.3150\\nSDM-v2.0 [48] 1.28B 865M 27.48 26.89 26.86 27.27 27.13 0.5065 0.4221 0.4922 0.1342 0.3096 0.3386 0.3672\\nDALLE-2 [44] 6.5B - 27.34 26.54 26.68 27.24 26.95 0.5750 0.5464 0.6374 0.1283 0.3043 0.3696 0.4268\\nSDXL-Base-1.0 [41] 3.46B 2.6B 27.69 27.44 27.50 28.29 27.73 0.6369 0.5408 0.5637 0.2032 0.3110 0.4091 0.4441\\nBK-SDM-S [26] 655M 483M 26.64 26.77 26.87 26.61 26.72 0.3984 0.3783 0.4225 0.0731 0.3003 0.3695 0.3237\\nOurs-SDM-S 655M 483M26.73 26.95 27.00 26.74 26.86 0.43860.3950 0.4549 0.0832 0.3007 0.3777 0.3417\\nBK-SDM-B [26] 752M 580M 27.01 26.64 27.06 26.63 26.84 0.4192 0.4096 0.4409 0.0979 0.3077 0.3052 0.3301\\nOurs-SDM-B 752M 580M26.79 27.12 27.11 26.79 26.95 0.44360.4338 0.4680 0.1077 0.3090 0.3872 0.3582\\nBK-SDXL-700M 1.68B 782M 27.59 27.13 27.17 27.14 27.26 0.5202 0.4506 0.4564 0.1360 0.3008 0.3699 0.3723\\nKOALA-700M 1.68B 782M27.65 27.28 27.58 27.21 27.43 0.50680.4731 0.4674 0.1535 0.3008 0.3731 0.3791\\nBK-SDXL-1B 2.06B 1.16B 27.52 26.90 27.17 26.87 27.12 0.4876 0.4498 0.4578 0.1551 0.3035 0.3777 0.3719\\nKOALA-1B 2.06B 1.16B27.73 27.26 27.61 27.16 27.44 0.52230.5108 0.4864 0.1563 0.3019 0.3697 0.3912\\nTable 4. Visual aesthetics evaluationusing HPSv2 [67] (Left) andImage-text alignment evaluationusing T2I-CompBench [21] (Right).\\nNote that BK-SDXL-1B/700M are implemented by ourselves using our efficient U-Net with the BK-SDM [26]’s distillation method. For\\nfair comparison with other methods, we use 50 denoising steps with Euler discrete scheduler [24] same as the huggingface’s SDXL-Base-\\n1.0 model [66].\\nDW-1&UP-3) consistently boots performance, and espe-\\ncially LF at DW1&UP3 shows the best. Interestingly,\\nboth +LF all and +Res all are worse than the ones\\nat only DW-1&UP-3 and SA+CA+Res+FFN+LF all is\\nalso not better, demonstrating that the SA features are not\\ncomplementary to the other features.\\nWith these findings, we build aKnOwledge-distillAtion-\\nbased LAtent diffusion model with our efficient U-Net,\\ncalled KOALA. We train our KOALA models with the fol-\\nlowing objectives: Ltask + LoutKD + LfeatKD where we ap-\\nply our findings to LfeatKD. As shown in Tab. 2, we de-\\nsign two models, KOALA-1B and KOALA-700M, based\\non SDXL [41], with U-Net model sizes of 1.16B and 782M,\\nrespectively.\\n5. Experiments\\n5.1. Implementation details\\nDataset. Since the dataset used to train SDXL [41] is not\\npublicly available (i.e., internal data), we train the proposed\\nefficient U-Net in SDXL on publicly available LAION-\\nAesthetics V2 6+ [55, 57, 58] for reproducibility. As the\\ndataset contains some blank text and corrupted images, we\\nfilter the data and collect 8,483,623 image-text pairs.\\nTraining. We use the officially released SDXL-Base-\\n1.0 [40] and the training settings, while only replacing\\nits U-Net with our efficient U-Net. We use the same\\ntwo text encoders used in SDXL, which are OpenCLIP\\nViT-bigG [22] and CLIP ViT-L [43]. For V AE, we use\\nsdxl-vae-fp16-fix [4], which enables us to use FP16\\nprecision for V AE computation. We initialize the weights\\nof our U-Net with the teacher’s U-Net weights at the same\\nblock location. We train our KOALA models for 100K\\niterations with a batch size of 128 using four NVIDIA\\nA100 (80GB) GPUs. More details are described in A. For\\na fair comparison to our counterpart BK-SDM [26], we\\ntrain our efficient U-Nets with their distillation method un-\\nder the same data setup ( e.g., BK-SDXL-1B and -700M\\nin Tab. 4). Furthermore, we also train SDM-Base and SDM-\\nSmall in BK-SDM [26] with our approach (Ours-SDM-\\nBase & Ours-SDM-Small in Tab. 4), following the BK-\\nSDM training recipe.\\nEvaluation metric. Recently, several works [2, 41, 67,\\n68] have claimed that FID [17] is not closely correlated\\nwith visual fidelity because a feature extractor for FIDSDM training recipe.\\nEvaluation metric. Recently, several works [2, 41, 67,\\n68] have claimed that FID [17] is not closely correlated\\nwith visual fidelity because a feature extractor for FID\\nis pre-trained on the ImageNet [11] dataset, which does\\nnot overlap much with the datasets used to train recent\\ntext-to-image models ( e.g., style, types, resolution, etc.).\\nTherefore, instead of FID, we use Human Preference\\nScore (HPSv2) [67] as a visual aesthetics metric, which\\nallows us to evaluate visual quality in terms of more spe-\\ncific types. For image-text alignment, we use the T2I-\\ncompbench [21], which is a more comprehensive bench-\\nmark for evaluating the compositional text-to-image gener-\\nation capability than the single CLIP score [16].\\n5.2. Main results\\nVisual aesthetics. We compare our KOALA-700M/1B\\nmodels against state-of-the-art text-to-image models, in-\\ncluding popular open-sourced Stable diffusion models se-\\nries [41, 47, 48] and DALLE-2 [44]. Tab. 4 summarizes the\\nresults. Our KOALA-700M & KOALA-1B models based\\non SDXL [41] consistently achieve a higher HPS average\\nscore than the BK [26] models (BK-SDXL-700M & 1B)\\nequipped with our efficient U-Net. Moreover, for SDM,\\nOurs-SDM-Base & Smalll models using BK’s compressed\\nU-Net in SDM-v1.4 [47] still outperform BK-SDM-Base &\\nSmalll [26]. These results demonstrate that the proposed\\ndistillation of the self-attention layer is more helpful for\\nvisual aesthetics than the last layer feature distillation by\\nBK [26]. In addition, our KOALA models achieve a higher\\nquality score than DALLE-2 [44], which has a much larger\\nmodel size (6.5B). Furthermore, our KOALA-700M sur-\\npasses SDM-v2.0 [48] with a comparable U-Net size, which\\nis widely used in the community. In Appendix C, we pro-\\nvide the qualitative comparisons to DALLE-2, SDM-v2.0,\\n7OOM/OOMOOM/6.3GBOOM/7.2GBOOM/7.7GBOOM/10.5GB9.2GB/6.1GBOOM/9.7GB10.8GB/8.9GB22.8GB/13.0GB10.8GB/6.6GB17.3GB/10.2GB15.7GB/9.5GBMemory:\\nFigure 5. Latency and memory usage comparison on different GPUs: NVIDIA 3060Ti (8GB), 2080Ti (11GB), and 4090 (24GB). OOM\\nmeans Out-of-Memory. We measure the inference time of SDM-v2.0 with 768 × 768 resolution and the other models with 1024 × 1024.\\nWe use 25 denoising steps and FP16/FP32 precisions. Note that SDXL-Base cannot operate in the 8GB-GPU.\\nLimitations (Ours-700M)\\nA 3d art character of a cute kitty holding a sign that says\"Let there be peace\", pixar styleA baby penguinwearing a blue hatand red gloves\\nFigure 6. Failure cases of KOALA-700M\\nand SDXL-Base-1.0, supporting the quantitative results.\\nImage-text alignment. As shown in Tab. 4 (Right), our ap-\\nproach with both SDXL and SDM consistently surpasses\\nthe counterpart BK method [26] in terms of text-image\\nalignment. We conjecture that this is because our self-\\nattention-based KD approach allows the model to learn\\nmore discriminative representations between objects or at-\\ntributes, as demonstrated in Sec. 4.2 and Fig. 4. Mean-\\nwhile, unlike the aesthetics results (HPSv2), our models\\nlag behind DALLE-2 regarding the average score of the\\nCompBench. In attribute binding (color, shape, and tex-\\nture), our model lags behind DALLE-2 but outperforms in\\nobject-relationship metrics. We speculate that the different\\ntendency between DALLE-2 and our model may stem from\\ndata used for training. Because the LAION-Aesthetics V2\\n6+ [57] data we used focuses on higher aesthetic images\\nthan multiple objects with various attributes, our model is\\nvulnerable to texts with different attribute properties.\\n5.3. Model budget comparison\\nWe further validate the efficiency of our model by mea-\\nsuring its inference speed on a variety of consumer-grade\\nGPUs with different memory sizes, such as 8GB (3060Ti),\\n11GB (2080Ti), and 24GB (4090), because the GPU en-\\nvironment varies for each user. Fig. 5 illustrates infer-\\nence speed and GPU memory usage on different GPUs\\nwith both FP16 and FP32 precisions. For this experiment,\\nwe compare against the most popular open-sourced mod-\\nels, SDM-v2.0 [48] and SDXL-Base-1.0 [41]. On the 8GB\\nGPU, SDXL does not fit , but the other models can run in\\nFP16 precision. Notably, KOALA-700M generates higher-\\nresolution images of superior quality at a comparable in-\\nference speed to SDM-v2.0. On the 11GB GPU, SDXL\\ncan run with FP16 precision, and on the 24 GB, it can run\\nat 9.66s and 3.3s with both FP16 and FP32 precision, re-\\nspectively. On the other hand, our KOALA-700M runs at\\n3.94s and 1.42s, which is 2× faster than SDXL. Overall,\\nour KOALA-700M is the best alternative for high-quality\\nimage generation that can replace SDM-v2.0 and SDXL in\\nresource-constrained GPU environments.\\n6. Limitations and Future Work\\nWhile our KOALA models generate images with impres-\\nsive aesthetic quality, such as the photo-realistic or 3d-art\\nrenderings shown in Fig. 1, it still shows limitations in sev-\\neral specific cases:\\nRendering long legible text. Our models have difficulty in\\nsynthesizing legible texts in the generated image. For exam-\\nple, it renders unintended letters or generates unintelligible\\nletters, as shown in Fig. 6 (Left).\\nComplex prompt with multiple attributes. When at-\\ntempting to compose an image using prompts that include\\nvarious attributes of an object or scene, KOALA sometimes\\ngenerates instances that do not perfectly follow the intended\\ndescription. For example, as shown in Fig. 6 (Right), when\\nwe configure the penguin to wear a blue hat and red gloves,\\nonly the blue hat attribute is applied, while the red gloves\\nare not.\\n8We conjecture that these limitations may stem from the\\ndataset, LAION-aesthetics-V2 6+ [57], we used to train,\\nwhose text prompts are relatively shorter (lacking detail)\\nand messy ( e.g., HTML code). Recent works [1, 69] also\\npointed out this issue and showed that utilizing machine-\\ngenerated detailed captions ( i.e., synthesized captions) im-\\nproves the fine-grained text-alignment of T2I models. For\\nfuture work, it will also be interesting to see the synergies\\nbetween our efficient T2I model and such large multimodal\\nmodels [5, 33, 34]-based recaptioning techniques. More\\nfailure cases are illustrated in Appendix C.4.\\n7. Conclusion\\nIn this work, we propose KOALA, an efficient text-to-image\\nsynthesis model, offering a compelling alternative between\\nSDM-v2.0 and SDXL in resource-limited environments. To\\nachieve this, we devise more compact U-Nets by effectively\\ncompressing the main computational bottlenecks present\\nin SDXL. In doing so, we demonstrate that self-attention-\\nbased knowledge distillation is one of the most crucial com-\\nponents to enhance the quality of generated images. With\\nthese contributions, our KOALA-700M model substantially\\nreduces the model size (69% ↓) and the latency (60% ↓) of\\nSDXL, while exhibiting decent aesthetic generation quality.\\n8. Acknowledgments\\nThis work was supported by Institute of Information &\\ncommunications Technology Planning & Evaluation (IITP)\\ngrant funded by the Korea government (MSIT) (No. RS-\\n2022-00187238, Development of Large Korean Language\\nModel Technology for Efficient Pre-training).\\nReferences\\n[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\\nYunxin Jiao, and Aditya Ramesh. Improving image genera-\\ntion with better captions. https://cdn.openai.com/\\npapers/dall-e-3.pdf, 2023. 9\\n[2] Eyal Betzalel, Coby Penso, Aviv Navon, and Ethan Fe-\\ntaya. A study on the evaluation of generative models. arXiv\\npreprint arXiv:2206.10935, 2022. 2, 7\\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\\nAlign your latents: High-resolution video synthesis with la-\\ntent diffusion models. In CVPR, 2023. 1\\n[4] Ollin Boer Bohan. Sdxl-vae-fp16-fix. https :\\n//huggingface.co/madebyollin/sdxl- vae-\\nfp16-fix, 2023. 3, 7, 12\\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\\nguage models are few-shot learners. Advances in neural in-\\nformation processing systems, 33:1877–1901, 2020. 9\\n[6] Hanting Chen, Yunhe Wang, Chang Xu, Chao Xu, and\\nDacheng Tao. Learning student networks via feature embed-\\nding. IEEE Transactions on Neural Networks and Learning\\nSystems, 32(1):25–35, 2020. 5\\n[7] Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang,\\nChuo-Ling Chang, Andrei Kulik, and Matthias Grundmann.\\nSpeed is all you need: On-device acceleration of large\\ndiffusion models via gpu-aware optimizations. In CVPR-\\nWorkshop, 2023. 1\\n[8] Jooyoung Choi, Yunjey Choi, Yunji Kim, Junho Kim, and\\nSungroh Yoon. Custom-edit: Text-guided image editing with\\ncustomized diffusion models. In CVPR-Workshop, 2023. 1\\n[9] Benjamin Consolvo. Text-to-image stable diffusion with sta-\\nbility ai and compvis on the latest intel gpu. https://\\nmedium.com/intel-analytics-software , 2022.\\n1\\n[10] DeepFloyd. Deepfloyd. https://www.deepfloyd.\\nai/, 2022. 3\\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR, 2009. 7\\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\\nAn image is worth one word: Personalizing text-to-image\\ngeneration using textual inversion. In ICLR, 2023. 1\\n[13] Jianping Gou, Baosheng Yu, Stephen J Maybank, and\\nDacheng Tao. Knowledge distillation: A survey. Interna-generation using textual inversion. In ICLR, 2023. 1\\n[13] Jianping Gou, Baosheng Yu, Stephen J Maybank, and\\nDacheng Tao. Knowledge distillation: A survey. Interna-\\ntional Journal of Computer Vision, 2021. 5\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In CVPR,\\n2016. 4\\n[15] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, No-\\njun Kwak, and Jin Young Choi. A comprehensive overhaul\\nof feature distillation. In ICCV, 2019. 1, 3, 5\\n[16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\\nand Yejin Choi. Clipscore: A reference-free evaluation met-\\nric for image captioning. In EMNLP, 2021. 7\\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\\ntwo time-scale update rule converge to a local nash equilib-\\nrium. In NeurIPS, 2017. 2, 7\\n[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\\ning the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531, 2015. 1, 5\\n[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion\\nguidance. arXiv preprint arXiv:2207.12598, 2022. 12, 13,\\n15, 16, 17, 18\\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\\nsion probabilistic models. In NeurIPS, 2020. 3, 5, 12\\n[21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xi-\\nhui Liu. T2i-compbench: A comprehensive benchmark\\nfor open-world compositional text-to-image generation. In\\nNeurIPS, 2023. 2, 7\\n[22] Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini,\\nRohan Taori, Achal Dave, Vaishaal Shankar, Hongseok\\nNamkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi,\\nand Ludwig Schmidt. Openclip. https://github.\\ncom/mlfoundations/open_clip, 2021. 3, 7, 12\\n9[23] Ian T Jolliffe and Jorge Cadima. Principal component analy-\\nsis: a review and recent developments. Philosophical trans-\\nactions of the royal society A: Mathematical, Physical and\\nEngineering Sciences, 374(2065):20150202, 2016. 6, 12\\n[24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\\nElucidating the design space of diffusion-based generative\\nmodels. NeurIPS, 2022. 2, 7, 12, 13, 15, 16, 17, 18\\n[25] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\\nvosyan, Roberto Henschel, Zhangyang Wang, Shant\\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\\nto-image diffusion models are zero-shot video generators. In\\nICCV, 2023. 1\\n[26] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and\\nShinkook Choi. On architectural compression of text-to-\\nimage diffusion models. arXiv preprint arXiv:2305.15798,\\n2023. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14\\n[27] Diederik P Kingma and Max Welling. Auto-encoding varia-\\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3, 5\\n[28] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-\\ntiana, Joe Penna, and Omer Levy. Pick-a-pic: An open\\ndataset of user preferences for text-to-image generation. In\\nICCV, 2023. 2\\n[29] Nicholas Kolkin, Jason Salavon, and Gregory\\nShakhnarovich. Style transfer by relaxed optimal transport\\nand self-similarity. In CVPR, 2019. 5\\n[30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\\nShechtman, and Jun-Yan Zhu. Multi-concept customization\\nof text-to-image diffusion. In CVPR, 2023. 1\\n[31] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\\nfusion: Text-to-image diffusion model on mobile devices\\nwithin two seconds. arXiv preprint arXiv:2306.00980, 2023.\\n1\\n[32] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\\ntext-to-3d content creation. In CVPR, 2023. 1\\n[33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning, 2023. 9\\n[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning, 2023. 9\\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101, 2017. 12\\n[36] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\\nprobabilistic model sampling in around 10 steps. Advances\\nin Neural Information Processing Systems , 35:5775–5787,\\n2022. 14\\n[37] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\\nOn distillation of guided diffusion models. In CVPR, 2023.\\n1, 3\\n[38] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\\nadapter: Learning adapters to dig out more controllable\\nability for text-to-image diffusion models. arXiv preprint\\narXiv:2302.08453, 2023. 1\\n[39] Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, and\\nQun Liu. Alp-kd: Attention-based layer projection for\\nknowledge distillation. In AAAI, 2021. 5\\n[40] Dustin Podell, Zion English, Kyle Lacey, Andreas\\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna,\\nand Robin Rombach. Stable-diffusion-xl-base-1.0. https:\\n/ / huggingface . co / stabilityai / stable -\\ndiffusion-xl-base-1.0 , 2023. 7, 12\\n[41] Dustin Podell, Zion English, Kyle Lacey, Andreas\\nBlattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and\\nRobin Rombach. Sdxl: Improving latent diffusion mod-\\nels for high-resolution image synthesis. arXiv preprint\\narXiv:2307.01952, 2023. 1, 2, 3, 4, 5, 7, 8, 12, 13\\n[42] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\\npreprint arXiv:2209.14988, 2022. 1\\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language super-\\nvision. In ICML, 2021. 3, 7, 12\\n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\\nand Mark Chen. Hierarchical text-conditional image gen-\\neration with clip latents. arXiv preprint arXiv:2204.06125,\\n2022. 3, 7, 13\\n[45] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-\\nstra. Stochastic backpropagation and approximate inference\\nin deep generative models. In ICML, 2014. 3, 5\\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\\nthesis with latent diffusion models. In CVPR, 2022. 1, 3, 5\\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj ¨orn Ommer. Stable-diffusion-\\nv1.4. https://github.com/CompVis/stable-\\ndiffusion, 2022. 1, 3, 4, 5, 7\\n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj ¨orn Ommer. Stable-diffusion-\\nv2.0. https://github.com/Stability- AI/\\nstablediffusion, 2022. 1, 3, 7, 8\\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\\nPatrick Esser, and Bj ¨orn Ommer. Stable-diffusion-\\nv2.0. https://huggingface.co/stabilityai/\\nstable-diffusion-2, 2022. 13, 15, 16, 17, 18\\n[50] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,\\nAntoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:\\nHints for thin deep nets. arXiv preprint arXiv:1412.6550 ,\\n2014. 3, 5\\n[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\\nnet: Convolutional networks for biomedical image segmen-\\ntation. In Medical Image Computing and Computer-Assisted\\nIntervention–MICCAI 2015: 18th International Conference,\\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\\n18, pages 234–241. Springer, 2015. 3, 5\\n[52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\\ntuning text-to-image diffusion models for subject-driven\\ngeneration. In CVPR, 2023. 1, 14\\n10[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\\net al. Photorealistic text-to-image diffusion models with deep\\nlanguage understanding. 2022. 3\\n[54] Tim Salimans and Jonathan Ho. Progressive distillation for\\nfast sampling of diffusion models. In ICLR, 2022. 1, 3\\n[55] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\\nman, et al. Laion-aestheics v2. https://laion.ai/\\nblog/laion-aesthetics/, 2022. 7\\n[56] Christoph Schuhmann, Romain Beaumont, Richard\\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\\nTheo Coombes, Aarush Katta, Clayton Mullis,\\nMitchell Wortsman, et al. Laion-aesthetics v2\\n6.5+. https : / / huggingface . co / datasets /\\nChristophSchuhmann / improved _ aesthetics _\\n6.5plus, 2022. 12\\n[57] Christoph Schuhmann, Romain Beaumont, Richard\\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\\nTheo Coombes, Aarush Katta, Clayton Mullis,\\nMitchell Wortsman, et al. Laion-aesthetics v2\\n6+. https : / / huggingface . co / datasets /\\nChristophSchuhmann / improved _ aesthetics _\\n6plus, 2022. 2, 7, 8, 9, 12\\n[58] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\\nman, et al. Laion-5b: An open large-scale dataset for training\\nnext generation image-text models. In NeurIPS, 2022. 7\\n[59] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and\\nYan Yan. Post-training quantization on diffusion models. In\\nCVPR, 2023. 1\\n[60] Eli Shechtman and Michal Irani. Matching local self-\\nsimilarities across images and videos. In CVPR, 2007. 5\\n[61] Jiaming Song, Chenlin Meng, and Stefano Ermon.\\nDenoising diffusion implicit models. arXiv preprint\\narXiv:2010.02502, 2020. 3, 12, 13, 15, 16, 17, 18\\n[62] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali\\nDekel. Splicing vit features for semantic appearance transfer.\\nIn CVPR, 2022. 5\\n[63] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\\nDekel. Plug-and-play diffusion features for text-driven\\nimage-to-image translation. In CVPR, 2023. 1, 6\\n[64] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\\nand Thomas Wolf. Diffusers: State-of-the-art diffusion\\nmodels. https://github.com/huggingface/\\ndiffusers, 2022. 12\\n[65] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\\nand Thomas Wolf. Diffusers: State-of-the-art diffusion\\nmodels. https://github.com/huggingface/\\ndiffusers / blob / main / examples / text _ to _\\nimage/train_text_to_image_sdxl.py, 2023. 12\\n[66] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pe-\\ndro Cuenca, Nathan Lambert, Kashif Rasul, Mishig\\nDavaadorj, and Thomas Wolf. Stabilityai: Sdxl-base-\\n1.0. https://huggingface.co/stabilityai/\\nstable-diffusion-xl-base-1.0 , 2023. 2, 7, 13,\\n15, 16, 17\\n[67] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng\\nZhu, Rui Zhao, and Hongsheng Li. Human preference score\\nv2: A solid benchmark for evaluating human preferences of\\ntext-to-image synthesis. arXiv preprint arXiv:2306.09341 ,\\n2023. 2, 7, 13\\n[68] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hong-\\nsheng Li. Better aligning text-to-image models with human\\npreference. In ICCV, 2023. 2, 7\\n[69] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\\nfei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-\\nsive models for content-rich text-to-image generation. arXiv\\npreprint arXiv:2206.10789, 2022. 9\\n[70] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-\\ncic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-\\ntent point diffusion models for 3d shape generation. arXiv\\npreprint arXiv:2210.06978, 2022. 1\\n[71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\\nconditional control to text-to-image diffusion models. Inpreprint arXiv:2210.06978, 2022. 1\\n[71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\\nconditional control to text-to-image diffusion models. In\\nICCV, 2023. 1\\n11Appendix\\nA. Implementation details\\nA.1. Training\\nWe use the officially released SDXL-Base-1.0 [40] with\\nDiffusers library [64, 65], while only replacing its\\nU-Net with our efficient U-Net. We use the same two\\ntext encoders used in SDXL, which are OpenCLIP ViT-\\nbigG [22] and CLIP ViT-L [43]. For V AE, we use\\nsdxl-vae-fp16-fix [4], which enables us to use FP16\\nprecision for V AE computation. We initialize the weights\\nof our U-Net with the teacher’s U-Net weights at the same\\nblock location. We freeze the text encoders, V AE, and\\nthe teacher U-Net of SDXL and only fine-tune our U-Net.\\nWe train our KOALA models on LAION-Aesthetics V2\\n6+ [57] dataset (about 800M text-image pairs) for 100K\\niterations using four NVIDIA A100 (80GB) GPUs with a\\nresolution of 1024 × 1024, a discrete-time diffusion sched-\\nule [20], size- and crop-conditioning as in SDXL [41], a\\nbatch size of 128, AdamW optimizer [35], a constant learn-\\ning rate of 10−5, and FP16 precision. For a fair compar-\\nison to our counterpart BK-SDM [26], we train our effi-\\ncient U-Nets with their distillation method under the same\\ndata setup (e.g., BK-SDXL-1B and -700M in Tab. 4). Fur-\\nthermore, we also train SDM-Base and SDM-Small in BK-\\nSDM [26] with our approach (Ours-SDM-Base & Ours-\\nSDM-Small in Tab. 4), following the BK-SDM training\\nrecipe on LAION-Aesthetics V2 6.5+ [56]. For the ablation\\nstudies in Tab. 3, we train all models for 30K iterations with\\na batch size of 32 on LAION-Aesthetics V2 6.5+ datasets\\nfor fast verification.\\nA.2. Inference\\nWhen generating samples, we also use FP16-precision and\\nsdxl-vae-fp16-fix [4] for V AE-decoder. Note that\\nin the SDXL original paper [41], authors used DDIM sam-\\npler [61] to generate samples in the figures while the dif-\\nfuser’s official SDXL code [65] used Euler discrete sched-\\nuler [24] as the default scheduler. Therefore, we also use\\nthe Euler discrete scheduler for generating samples. With\\nthe Euler discrete scheduler, we set the denoising step to 50\\nonly for quantitative evaluation in Tab. 3 and Tab. 4, and\\nset it to 25 for other qualitative results or latency measure-\\nments. we set classifier-free guidance [19] to 7.5.\\nA.3. Implementation details of SA-bottom\\nFig. 7 illustrates how to choose transformer blocks when\\ndistilling self-attention (SA) features at DW3 & MID &\\nUP1 as described in Sec. 4.2 (F.3) and Tab. 3c. In Fig. 7,\\nthe Transformer blocks (yellow) with a depth of 10 is from\\nthe original SDXL’s U-Net teacher model, and the Trans-\\nformer blocks (blue) with a depth of 6 is from our KOALA-\\nT0T1T2T3T4T5T6T7T8T9\\nTransformer blocks (depth=10)\\nT0T1T2T3T4T5\\nTransformer blocks (depth=6)Transformer block\\nSACAFF\\nFigure 7. SA-bottom illustration in Tab. 3c.\\n1B’s U-Net student model. For SA-bottom in Tab. 3c,\\nwe perform feature distillation by selecting consecutive\\nblocks from the teacher model’s transformer blocks, start-\\ning with the first one, and comparing to each transformer’s\\nself-attention (SA) features from the student model’s trans-\\nformer blocks.\\nB. Additional Analysis\\nB.1. Attention visualization for Tab. 3a and Tab. 3b\\nIn Section 4.3 of the main paper, we provide empirical\\nevidence demonstrating the paramount importance of self-\\nattention features in the distillation process. Our findings\\nparticularly highlight the significant impact of specific self-\\nattention (SA) stages (e.g., UP-1&UP-2) on enhancing per-\\nformance. To support these results, we extensively ana-\\nlyze self-attention maps in the main paper. To complete\\nthe analysis, we expand our Principal Component Analy-\\nsis [23] (PCA) on self-attention maps to encompass all lay-\\ners in Fig. 9.\\nAs elaborated in the main paper, self-attention begins by\\ncapturing broad contextual information (e.g., DW-2&DW-3)\\nand then progressively attends to localized semantic de-\\ntails ( e.g., MID). Within the decoder, self-attentions are\\nincreasingly aligned with higher-level semantic elements\\nUP-1&UP-2), such as objects, for facilitating a more ac-tails ( e.g., MID). Within the decoder, self-attentions are\\nincreasingly aligned with higher-level semantic elements\\nUP-1&UP-2), such as objects, for facilitating a more ac-\\ncurate representation of appearances and structures. No-\\ntably, at this stage, the SA-based model focuses more on\\nspecific object regions than the LF-based model. This leads\\nto a marked improvement in compositional image genera-\\ntion performance.\\n120.0 0.2 0.4 0.6 0.8 1.0\\nNormalized Layer Index\\n0.8\\n0.85\\n0.9\\n0.95\\n1.0\\nFeature Cosine Similarity\\nDW\\nMID\\nUP\\nFigure 8. Feature cosine similarity analysis. We plot the cross-\\nlayer cosine similarity against the normalized layer indexes of\\ntransformer block.\\nB.2. Feature cosine similarity analysis for Tab. 3c\\nKOALA models compress the computationally inten-\\nsive transformer blocks in the lowest feature levels ( i.e.,\\nDW-3&Mid&UP-1 stages). Specifically, we reduce the\\ndepth of these transformer blocks from 10 to 5 for KOALA-\\n1B and to 6 for KOALA-700M. For this purpose, we\\ndemonstrate that distilling knowledge from the consecutive\\nbottom layers of transformer blocks is a simple yet effective\\nstrategy (see third finding (F3) in the main paper).\\nTo delve deeper into the rationale behind this strategy,\\nwe conducted a thorough feature analysis of the original\\nSDXL model [41]. In particular, we investigate the evo-\\nlution of the features within the transformer blocks. We\\ncompute the cross-layer cosine similarity between the out-\\nput features of each block and those of its predecessors. A\\nlower similarity score indicates a significant contribution of\\nthe current block, whereas a higher score implies a marginal\\ncontribution.\\nFor this analysis, we leverage the diverse domain of\\nprompts in the HPSv2 dataset [67]. We compute the cross-\\nlayer cosine similarity for each stage ( DW&Mid&UP) and\\naverage these values across all prompts. The results are il-\\nlustrated in Fig. 8. For all stages, transformer blocks ex-\\nhibit a tendency of feature saturations: While early trans-\\nformer blocks generally show a significant contribution,\\nlater blocks have less impact. This motivates us to distill the\\nlearned knowledge of consecutive bottom layers of trans-\\nformer blocks for minimal performance degradation.\\nC. Qualitative results\\nC.1. Comparison to other methods\\nWe show generated samples with various types of prompt\\nstyle (e.g., portrait photo, 3d art animation, and paintings)\\ncomparing with DALLE-2 [44], SDM-v2.0 1 and SDXL 2\\nin Figs. 10 to 12. When generating samples, we set the\\nsame random seed for all models except DALLE-2 because\\nfor DALLE-2 API, we cannot set the random seed. Over-\\nall, our method surpasses DALLE-2 and SDM-v2.0 in terms\\nof visual aesthetic quality and demonstrates decent results\\nwhen compared to SDXL.\\nWe also compare our model to Stable diffusion mod-\\nels, SDM-v2.0 and SDXL, with four random seeds\\nin Fig. 13. We follow the official inference setups of each\\nmodel (SDM-v2.0 [49] and SDXL-Base-1.0 [66]) using\\nhuggingface. Specifically, SDM-v2.0 is set to gener-\\nate with DDIM scheduler [61] with 25 steps and SDXL and\\nours are set to use Euler discrete scheduler [24] with 25\\nsteps. And we set all models to use the classifier-free guid-\\nance [19] with 7.5. The results highlight the robustness of\\nthe KOALA model against varying random seed selection.\\nC.2. Comparison to BK-SDM\\nIn addition to the quantitative comparisons in the main pa-\\nper, we also provide a qualitative comparison with BK-\\nSDM [26]. As illustrated in Fig. 14, BK-SDM occasionally\\noverlooks specific attributes or objects mentioned in the text\\nprompt and generates structurally invalid images. On the\\ncontrary, our proposed model consistently generates images\\nwith enhanced adherence to the text, showcasing a superior\\nability to capture the intended details accurately.\\nC.3. Self-Comparison on KOALA models\\nKOALA-1B vs. KOALA-700M To assess the influence\\nof model size on performance, we conducted a compara-\\ntive analysis between KOALA-1B and KOALA-700M. The\\nresults, as illustrated in Fig. 15 and Fig. 16, reveal that\\nKOALA-1B is able to capture finer details and exhibits a\\nmarginally superior adherence to text prompts. However,\\ndespite its smaller size, KOALA-700M still delivers im-\\npressive results, with the added advantage of a significantly\\nfaster inference time.\\nControllability of KOALA The KOALA-700M model ex-\\nhibits not only faithful visual quality but also remark-pressive results, with the added advantage of a significantly\\nfaster inference time.\\nControllability of KOALA The KOALA-700M model ex-\\nhibits not only faithful visual quality but also remark-\\nable controllability in response to the given text prompts.\\nFig. 17 highlights the impressive controllability for chal-\\nlenging cases, including various painter’s styles (see the first\\nrow), diverse color reflections (see the second row), a range\\nof seasons (see the third row), and different times of day\\n(see the fourth row).\\nC.4. Failure cases\\nAs noted in the main paper, the KOALA models have cer-\\ntain limitations despite their great aesthetic quality. To thor-\\n1https://huggingface.co/stabilityai/stable-diffusion-\\n2\\n2https://huggingface.co/stabilityai/stable-diffusion-\\nxl-base-1.0\\n13“A kitten and a dog sat side by side” \\nLF (BK)SA (Ours)\\n(a) Generated Image (b) PCA Analysis\\nDW2 DW3 MID UP1 UP2\\n“a big hippopotamus and a small rabbit.” \\nLF (BK)SA (Ours)\\n(a) Generated Image (b) PCA Analysis\\nDW2 DW3 MID UP1 UP2\\nFigure 9. Extended analysis on self-attention maps of distilled student U-Nets. (a) Generated images of LF- and SA-based distilled\\nmodels, which are BK-SDM [26] and our proposal, respectively. In BK-SDM’s result, a rabbit or dog is depicted like a hippopotamus or\\ncat, repectively (i.e., appearance leakage). (b) Visualization of PCA analysis results. Note that from the UP-1 stage, the SA-based model\\nattends to the corresponding object (i.e., rabbit or dog) more discriminatively than the LF model, demonstrating that self-attention-based\\nKD allows to generate objects more distinctly.\\noughly understand these constraints, we present additional\\nexamples and categorize them into distinct cases. Fig. 18\\nclearly demonstrates that the KOALA-700M model faces\\nchallenges in complex scenarios, such as complex com-\\npositional prompts with multiple attributes (the first row),\\nrendering legible text (the second row), capturing intricate\\nstructural details (the third row), and accurately depicting\\nhuman hands (the fourth row).\\nD. Downstream task: Dreambooth\\nTo validate the transferability and generation capability of\\nour KOALA model, we apply our KOALA-700M model\\nto a custom text-to-image (T2I) downstream task, Dream-\\nbooth [52], which is a popular custom model for person-\\nalized T2I generation. We fine-tune our KOALA-700M\\nmodel on the Dreambooth dataset using resizing 1024, the\\n8-bit Adam optimizer, a constant learning rate of 5e-5, and a\\nbatch size of 4 for 500 iterations without the incorporation\\nof a class-preservation loss. The number of steps for gra-\\ndient accumulation is set to 2. For generating images, we\\nuse DPM-Solver [36] with 25 denoising steps. As shown\\nin Fig. 19, with about 5-6 photographs provided, subject\\ntraining is conducted alongside an identifier token, taking\\napproximately 20 minutes per subject on an NVIDIA RTX\\nA6000 GPU. The results demonstrate that the images are\\ngenerated seamlessly, without any inconsistencies between\\nthe text and the object.\\n14SDXL-Base-1.0KOALA-700M (ours)SDM-v2.0DALLE-2Close-up face photography of an old man standing in the rainat night, in the street lit by lamps, leica35mm summilux\\nClose-up facephotography of a girlstanding in the snow at noon, in the street lit by lamps\\nAphoto of an adorable girl, front view, centered, soft lighting, outdoors\\nPortrait photo of a young asian boy smiling, highly detailed, 8k\\nPortrait photo of a standing girl,photograph, golden hair, depth of field, moody light, golden hour, centered, extremely detailed, realistic\\nFigure 10. Qualitative comparison with DALLE-2, SDM-v2.0, and SDXL in terms of portrait photo. We follow the official inference\\nsetups of each model (SDM-v2.0 [49] and SDXL-Base-1.0 [66]) using huggingface. Specifically, SDM-v2.0 is set to generate with\\nDDIM scheduler [61] with 25 steps and SDXL and ours are set to use Euler discrete scheduler [24] with 25 steps. And we set all models\\nto use the classifier-free guidance [19] with 7.5.\\n15A tiny cute puppytoy, riding a snowboard, standing character, in the galaxy, 3dblender render\\nA super cute and adorable puppy wearing a winter hat and scarf, fantasy, cartoon, disney, pixar, anime\\nA 3d art animationof a cute baby raccoon walking on Mars, wearing an astronaut suit, with many stars in the sky\\nKawaii low poly squirrel character, 3d isometric render, white background, ambient occlusion, unity engine\\nSDXL-Base-1.0KOALA-700M (ours)SDM-v2.0DALLE-2\\nA 3d art character of super tiny cute rabbit, the sunflowers garden, standing character, unreal engine, 3drender\\nFigure 11. Qualitative comparison with DALLE-2, SDM-v2.0, and SDXL in terms of 3D art . We follow the official inference setups\\nof each model (SDM-v2.0 [49] and SDXL-Base-1.0 [66]) using huggingface. Specifically, SDM-v2.0 is set to generate with DDIM\\nscheduler [61] with 25 steps and SDXL and ours are set to use Euler discrete scheduler [24] with 25 steps. And we set all models to use\\nthe classifier-free guidance [19] with 7.5.\\n16A portrait of Ludwig van Beethovenin the style of Vincent van Gogh’s “Starry Night” painting.\\nStippling sketchof a catwith its fur, eyes, and features meticulously crafted using thousands of tiny dots. The density of the dots varies, creating depth and shading throughout the artwork.\\nPhoto in a square composition of a graffiti-styled caton a brick wall, vibrant colors, bold strokes, and urban aesthetics come together to depict the feline in a dynamic and street art manner, extremely detailed.\\nSDXL-Base-1.0KOALA-700M (ours)SDM-v2.0DALLE-2\\nVector illustrationof living roomin flat style, pastelcolor palette\\nOil paintingof black holeand astronaut\\nFigure 12. Qualitative comparison with DALLE-2, SDM-v2.0, and SDXL in terms of painting. We follow the official inference setups\\nof each model (SDM-v2.0 [49] and SDXL-Base-1.0 [66]) using huggingface. Specifically, SDM-v2.0 is set to generate with DDIM\\nscheduler [61] with 25 steps and SDXL and ours are set to use Euler discrete scheduler [24] with 25 steps. And we set all models to use\\nthe classifier-free guidance [19] with 7.5.\\n17A cute magical flying dog, fantasy art, golden color, high quality, highly detailed, elegant, sharp focus, concept art, character concepts, digital painting, mystery, adventure\\nSDM-v2.0SDXLKOALA-700M\\nAn illustration of a robotic wolf, wearing sunglasses and hat, cold color, raining, dark, mist, smoke,extremely detailed, photorealistic\\nSDM-v2.0KOALA-700M\\nSDXL\\nFigure 13. Qualitative comparison between SDM-v2.0 vs. SDXL-Base-1.0 vs. KOALA-700M (ours). For each prompt, we use 4\\nrandom seeds to generate images, while all models are generated with the same seed for each image. SDM-v2.0 [49] is set to generate with\\nDDIM scheduler [61] and SDXL and ours are set to use Euler discrete scheduler [24]. All samples are generated with 25 denoising steps\\nand the cfg-scale [19] 7.5.\\n18The square boxwas next to the circular canisterThe rectangular mirrorwas hung above the blue sink\\nBK-SDXL-700MKOALA-700MBK-SDXL-700MKOALA-700M\\nA round muffinand a square napkin\\nA red backpackand a blue book A brown sheep toyand a blue vase\\nA dogis chasinga balland having funin the park A boatis sailing on a lakeand birdsare flyingabove\\nA round bageland a square toaster\\nFigure 14. Qualitative comparison between BK-Base-700M vs. KOALA-700M (ours).\\n19Cute Catin a Variety of Colors, Universe fulfilling the body, fantasy, renaissance aesthetic, Star Trek aesthetic, pastel colors aesthetic, intricate fashion clothing, highly detailed, surrealistic, digital painting, concept art, sharp focus, illustration\\nPencil paintingof young girl\\n Renaissance-styleportrait of an astronautin space, detailed starrybackground, reflective helmet\\nA 3d art character of a tiny cute rabbit, big reflect eyes, wearing a hoodie, in the city, full body shot, 3D, character, 3d rendering, realistic, adorable, physically based rendering Vibrant paintingof a flowerpunk owl, dramatic lighting, abstract flowers, hightly detailed digital painting, 8K\\nAbstract watercolor animeart of a magical girl surrounded by flowers, 8k, stunning intricate details, by artgermA blue haired girl, with blowing bubbles, with a sophisticated intellectual style, anime. dark, cold color\\nA vitral window, Art Noveaustyle, with colorful nature motives and a big foxin the middle, roundedupside, photorealistic\\nKOALA-1B KOALA-700MKOALA-1B KOALA-700M\\nFigure 15. Generated samples of KOALA-1B and KOALA-700M.\\n20A majestic eagleon topof a big treeat twilight\\nWall graffitiart of astronaut holding a super soakerImpressionist oil paintingof a beachat sunsetwith a narrow aspect ratio\\nAn aerial viewof a cityat night, long exposureLong-exposure nightphotography of a starry skyover a mountain range, with light trails, high detail\\nA sketch of a mysterious castlein the style of Gothicart with an aerial viewpointA Giant space battleship, small flying objects, stars, and nebulain the background, inspired by the movie Star Wars\\nA realistic photo of the astronautreading the bookon the mars, under the moon\\nKOALA-1B KOALA-700MKOALA-1B KOALA-700M\\nFigure 16. Generated samples of KOALA-1B and KOALA-700M.\\n21A portrait painting of a Golden Retriever like Leonard da VinciAportraitpaintingofaGoldenRetrieverlikeClaudMonet A portrait painting of a Golden Retriever like Andy Warhol\\nOil painting influenced by Monet\\'s impressionist style, presenting a sunrise over a harbor. The calm waters are bathed in a golden light from the sun, with distant silhouettes of anchored ships and boats. The sky transitions through soft hues of light pinks, greens, and yellows. The sun\\'s shimmering reflection on the water enhances the depth of the scene. The artwork is characterized by its loose, expressive brush strokes, embodying the serenity of a peaceful morning\\nOil painting influenced by Monet\\'s impressionist style, presenting a sunrise over a harbor. The calm waters are bathed in a golden light from the sun, with distant silhouettes of anchored ships and boats. The sky transitions through soft hues of bright blue, yellows, and reds. The sun\\'s shimmering reflection on the water enhances the depth of the scene. The artwork is characterized by its loose, expressive brush strokes, embodying the serenity of a peaceful morning\\nOil painting influenced by Monet\\'s impressionist style, presenting a sunrise over a harbor. The calm waters are bathed in a golden light from the sun, with distant silhouettes of anchored ships and boats. The sky transitions through soft hues of cool blues, white, and greys. The sun\\'s shimmering reflection on the water enhances the depth of the scene. The artwork is characterized by its loose, expressive brush strokes, embodying the serenity of a peaceful morning\\nA portrait painting of a Golden Retriever like Edvard Munch \"The Scream\"\\nA photo of a young girl in spring, black hair, front view, smiling, highly detailed, professional photographyA photo of a young girl in summer, black hair, front view, smiling, highly detailed, professional photographyA photo of a young girl in fall, black hair, front view, smiling, highly detailed, professional photographyA photo of a young girl in winter, black hair, front view, smiling, highly detailed, professional photography\\nA majestic lion standing in front of El Capitan in Yosemite National Park at morning A majestic lion standing in front of El Capitan in Yosemite National Park at noon A majestic lion standing in front of El Capitan in Yosemite National Park at twilight A majestic lion standing in front of El Capitan in Yosemite National Park at night\\nOil painting channeling Monet\\'s impressionist technique, presenting a sunrise over a harbor. The serene waters radiate with the sun\\'s golden light, and distant silhouettes of ships and boats are evident. The expansive sky is artfully painted with variations of a single purple shade. The sun\\'s shimmering reflection on the water adds depth and vibrancy to the scene. The artwork is marked by its loose, expressive brush strokes, conveying the tranquility of a peaceful morning\\nFigure 17. Generated samples of KOALA-700M. Our KOALA-700M model demonstrates faithful visual quality and remarkable con-\\ntrollability across various painter’s styles (see the first row), diverse color reflections (see the second row), a range of seasons (see the third\\nrow), and different times of day (see the fourth row), in response to the given text prompts.\\n22A corgiwearing a red bowtieand a purple party hatA white dog (left) and a black cat (right) are running together with smilingA close-up photo of a young boy with a white hat, blue t-shirt, and denim jacket\\nA neon sign that reads “Backprop”\\nA photo of a black handbagand a green walleton the wooden floor\\nA photo of the Burj Khalifa, extremely detailed, night, best quality, professional photography, light, strong contrast\\nA close-up photo of handfull of maple leaves.\\nA photo of a store with the name \"koala grocery store\" A picture of a sticky note on desk that says\"I can do it.\" A photo of a red sign that says \"No Trespassing\" in the desert.\\nA photo of the Tokyo Towerin the spring\\n A professional photo of a cultural heritage landscape with a traditional Korean house, in the background of a colorful sky, best quality, RAW photo, UHD, DSLR, soft lighting, high quality, film grain, night, light, strong contrast, contrast, glass reflection, high detailed building, building, time-lapse photography, indoor light, reflective, half aerial view, water, bridge\\nA professional photo of the Colosseumat sunset\\nA photorealistic image of a young girl blowing bubblesin a park, with colorful flowers and a big blue sky in the background. Shot from a close-up angle to capture the sense of playfulness and innocence\\nA photo of an old lady with her hand raisedin greeting.A woman is speaking on the phoneand making planswith a friend.\\nFigure 18. Failure cases of KOALA-700M. KOALA-700M model faces challenges in complex scenarios, such as complex compositional\\nprompts with multiple attributes (1st row), rendering legible text (2nd row), capturing intricate structural details (3rd row), and accurately\\ndepicting human hands (4th row).\\n23A [V]dog\\nA [V]dog floating on top of waterA [V]dog with a mountainin the background\\nA [V]dog in the jungleA [V]dog with the Eiffel Tower in the background\\nA [V]dog\\nA [V]dog floating on top of waterA [V]dog with a tree and autumnleaves in the background\\nA [V]dog on the beachA [V]dog with the Eiffel Tower in the background\\nSubject InputImage SampleGenerated Images\\nFigure 19. Image Generations with Dreambooth+KOALA-700M.\\n24'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = \"\"\n",
    "for item in pages:\n",
    "    docs += item.page_content\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c0193d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "template = \"\"\"\n",
    "'''\n",
    "{context}\n",
    "'''\n",
    "总结上面的论文内容\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "594bf7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这篇论文《**KOALA: Self-Attention Matters in Knowledge Distillation of Latent Diffusion Models for Memory-Efficient and Fast Image Synthesis**》提出了一种高效的文本到图像（T2I）生成模型KOALA，旨在解决当前最先进的**Stable Diffusion XL (SDXL)** 模型在计算资源和内存上的高昂开销问题。\\n\\n以下是论文的**核心内容总结**：\\n\\n---\\n\\n### 🎯 **研究目标**\\n- SDXL虽然质量高（1024×1024分辨率），但模型庞大（U-Net约2.56B参数）、推理慢、需要大显存（如24GB GPU），限制了其在消费级硬件上的部署。\\n- **目标**：在**不牺牲太多生成质量**的前提下，构建一个**更小、更快、节省内存的T2I模型**，作为SDXL的高效替代品。\\n\\n---\\n\\n### 🔍 **核心方法与创新点**\\n\\n#### ✅ 1. **深度分析SDXL的U-Net结构**\\n- 发现SDXL最耗资源的部分是**U-Net中的Transformer块**，尤其集中在低分辨率特征层（如32×32的特征图）。\\n- 这些Transformer块密集分布（尤其在DW-3、Mid、UP-1阶段），占用了绝大多数参数。\\n- 传统压缩方法（如BK-SDM）仅简单移除“块对”（residual + transformer），压缩率仅33%，效果有限。\\n\\n#### ✅ 2. **设计更高效的U-Net结构（KOALA-1B & KOALA-700M）**\\n- 基于上述分析，构建了两个轻量级U-Net：\\n  - **KOALA-1B**：1.16B参数（比SDXL小54%）\\n  - **KOALA-700M**：782M参数（比SDXL小69%，与SDM-v2.0相当）\\n- 压缩策略：\\n  - **移除部分Transformer块对**（尤其在解码阶段中间层）。\\n  - **降低底层Transformer块的深度**（从10个减少到5~6个）。\\n  - **在KOALA-700M中彻底移除Mid阶段的Transformer块**（进一步压缩）。\\n\\n> ✅ **效果**：模型大小大幅减小，同时保留关键生成能力。\\n\\n#### ✅ 3. **提出“基于自注意力（Self-Attention）的知识蒸馏”策略**\\n- 传统知识蒸馏（KD）主要对比预测的噪声（输出）或最后特征层，但**效果有限**。\\n- 论文的**核心发现**：**自注意力特征（SA）是知识蒸馏中最关键的部分**。\\n  - 自注意力机制能捕捉对象之间的语义关系和结构信息。\\n  - 蒸馏SA特征能让学生模型学习到更“有区分性”的表示，避免对象混淆（如把兔子画成河马）。\\n- 通过大量消融实验（如Tab. 3），验证了以下四点：\\n  1. **F1：SA特征 > 其他特征**（CA、FFN、Res）\\n  2. **F2：在解码器阶段蒸馏SA效果更好**（如UP-1）\\n  3. **F3：从Transformer块底部（early blocks）蒸馏效果最佳**（SA-bottom）\\n  4. **F4：在无Transformer的顶部阶段（DW-1/UP-3），蒸馏“最后特征”（LF）比蒸馏残差块（Res）更好**\\n\\n> ✅ **结论**：**自注意力是KD的“核心”**，不能忽视。\\n\\n---\\n\\n### 📊 **实验结果与性能对比**\\n\\n| 模型 | 参数量 | 相比SDXL | 推理速度 | 生成质量 |\\n|------|--------|----------|----------|----------|\\n| **SDXL-Base-1.0** | 2.56B（U-Net） | —— | 3.3秒（NVIDIA 4090） | 基准 |\\n| **KOALA-700M** | 782M（U-Net） | **↓69%** | ✅ **1.4秒（↓58%）** | **媲美或超过SDM-v2.0，接近SDXL** |\\n| **KOALA-1B** | 1.16B（U-Net） | **↓54%** | 1.6秒 | 优于BK-SDM，生成质量更高 |\\n\\n- **关键优势**：\\n  - 在8GB GPU上可运行（SDXL无法运行）。\\n  - 生成图像在**视觉美学（HPSv2）** 和 **图文对齐（T2I-CompBench）** 上优于BK-SDM。\\n  - **KOALA-700M速度 > SDXL的2倍**，且生成质量接近甚至超越SDM-v2.0（轻量级但低分辨率）。\\n\\n---\\n\\n### 🌟 **贡献总结**\\n1. **提出两种高效U-Net架构**：KOALA-1B & 700M，显著减小模型尺寸。\\n2. **首次系统性分析CD对SDXL的特征蒸馏**，揭示“**自注意力是最重要的蒸馏信号**”。\\n3. **构建高效T2I模型KOALA**，在资源受限设备（甚至8GB GPU）上运行，是SDXL和SDM-v2.0之间**理想平衡**的替代方案。\\n4. **提供了全面的性能评估和可复现训练流程**。\\n\\n---\\n\\n### 🧩 **局限与未来方向**\\n- 生成**可读文字**能力差（常生成乱码）。\\n- 对**多重属性组合**提示的控制能力较弱（如“穿蓝帽+红手套”的动物，只认到蓝帽）。\\n- 原因可能是训练数据（LAION-Aesthetics V2 6+）的**提示词较简短、质量参差**。\\n- 未来可结合**由大模型生成的高质量caption**进行再训练，提升分类/细粒度控制能力。\\n\\n---\\n\\n### 🏁 **一句话总结**\\n> **KOALA通过创新的U-Net压缩和以自注意力为核心的蒸馏策略，成功将SDXL的生成能力移植到仅782M参数的轻量模型中，在速度提升2倍的同时，仅损失少量质量，是开源T2I模型中**（尤其是资源受限环境）。\\n\\n---\\n\\n📌 **项目链接**：[https://youngwanlee.github.io/KOALA/](https://youngwanlee.github.io/KOALA/)  \\n✅ **代码/模型已开源**，适合部署在消费级GPU上。'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd9f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/zpc/langchain/blibli/data/animation.html'}, page_content='<!DOCTYPE html>\\n<html lang=\"zh\">\\n\\t<head>\\n\\t\\t<meta charset=\"utf-8\" />\\n\\t\\t<base href=\"../../../\" />\\n\\t\\t<script src=\"page.js\"></script>\\n\\t\\t<link type=\"text/css\" rel=\"stylesheet\" href=\"page.css\" />\\n\\t</head>\\n\\t<body>\\n\\t\\t<h1>动画系统（[name]）</h1>\\n\\n\\t\\t<h2>概述</h2>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t在three.js动画系统中，您可以为模型的各种属性设置动画：\\n\\t\\t\\t[page:SkinnedMesh]（蒙皮和装配模型）的骨骼，morph targets（变形目标），\\n\\t\\t\\t不同的材料属性（颜色，不透明度，布尔运算），可见性和变换。动画属性可以淡入、淡出、交叉淡化和扭曲。\\n\\t\\t\\t在相同或不同物体上同时发生的动画的权重和时间比例的变化可以独立地进行。\\n\\t\\t\\t相同或不同物体的动画也可以同步发生。\\n\\t\\t\\t<br /><br />\\n\\t\\t\\t为了在一个同构系统中实现所有这一切，\\n\\t\\t\\tthree.js的动画系统[link:https://github.com/mrdoob/three.js/issues/6881 在2015年彻底改变]（注意过时的信息！），\\n\\t\\t\\t它现在有一个与Unity/虚幻4引擎类似的架构。此页面简要阐述了这个系统中的主要组件以及它们如何协同工作。\\n\\t\\t</p>\\n\\n\\t\\t<h3>动画片段（Animation Clips）</h3>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t如果您已成功导入3D动画对象（无论它是否有骨骼或变形目标或两者皆有都不要紧）——\\n\\t\\t\\t例如使用[link:https://github.com/KhronosGroup/glTF-Blender-IO glTF Blender exporter]（glTF Blender导出器）</a>\\n\\t\\t\\t从Blender导出它并使用[page:GLTFLoader]将其加载到three.js场景中\\n\\t\\t\\t—— 其中一个响应字段应该是一个名为“animations”的数组，\\n\\t\\t\\t其中包含此模型的[page:AnimationClip AnimationClips]（请参阅下面可用的加载器列表）。\\n\\t\\t\\t<br /><br />\\n\\t\\t\\t每个*AnimationClip*通常保存对象某个活动的数据。\\n\\t\\t\\t举个例子，假如mesh是一个角色，可能有一个AnimationClip实现步行循环，\\n\\t\\t\\t第二个AnimationClip实现跳跃，第三个AnimationClip实现闪避等等。\\n\\t\\t</p>\\n\\n\\t\\t<h3>关键帧轨道（Keyframe Tracks）</h3>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t在这样的*AnimationClip*里面，每个动画属性的数据都存储在一个单独的[page:KeyframeTrack]中。\\n\\t\\t\\t假设一个角色对象有[page:Skeleton]（骨架），\\n\\t\\t\\t一个关键帧轨道可以存储下臂骨骼位置随时间变化的数据，\\n\\t\\t\\t另一个轨道追踪同一块骨骼的旋转变化，第三个追踪另外一块骨骼的位置、转角和尺寸，等等。\\n\\t\\t\\t应该很清楚，AnimationClip可以由许多这样的轨道组成。\\n\\t\\t\\t<br /><br />\\n\\t\\t\\t假设模型具有morph Targets（变形目标）——\\n\\t\\t\\t例如一个变形目标显示一个笑脸，另一个显示愤怒的脸 ——\\n\\t\\t\\t每个轨道都持有某个变形目标在AnimationClip运行期间产生的[page:Mesh.morphTargetInfluences]（变形目标影响）如何变化的信息。\\n\\t\\t</p>\\n\\n\\t\\t<h3>动画混合器（Animation Mixer）</h3>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t存储的数据仅构成动画的基础 —— 实际播放由[page:AnimationMixer]控制。\\n\\t\\t\\t你可以想象这不仅仅是动画的播放器，而是作为硬件的模拟，如真正的调音台，可以同时控制和混合若干动画。\\n\\t\\t</p>\\n\\n\\t\\t<h3>动画行为（Animation Actions）</h3>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t*AnimationMixer*本身只有很少的（大体上）属性和方法，\\n\\t\\t\\t因为它可以通过[page:AnimationAction AnimationActions]来控制。\\n\\t\\t\\t通过配置*AnimationAction*，您可以决定何时播放、暂停或停止其中一个混合器中的某个*AnimationClip*，\\n\\t\\t\\t这个*AnimationClip*是否需要重复播放以及重复的频率，\\n\\t\\t\\t是否需要使用淡入淡出或时间缩放，以及一些其他内容（例如交叉渐变和同步）。\\n\\t\\t</p>\\n\\n\\t\\t<h3>动画对象组（Animation Object Groups）</h3>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t如果您希望一组对象接收共享的动画状态，则可以使用[page:AnimationObjectGroup]。\\n\\t\\t</p>\\n\\n\\t\\t<h3>支持的格式和加载器（Supported Formats and Loaders）</h3>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t请注意，并非所有模型格式都包含动画（尤其是OBJ，没有），\\n\\t\\t\\t而且只有某些three.js加载器支持[page:AnimationClip AnimationClip]序列。\\n\\t\\t\\t以下几个<i>确实</i>支持此动画类型：\\n\\t\\t</p>\\n\\n\\t\\t\\t<ul>\\n\\t\\t\\t\\t<li>[page:ObjectLoader THREE.ObjectLoader]</li>\\n\\t\\t\\t\\t<li>THREE.BVHLoader</li>\\n\\t\\t\\t\\t<li>THREE.ColladaLoader</li>\\n\\t\\t\\t\\t<li>THREE.FBXLoader</li>\\n\\t\\t\\t\\t<li>[page:GLTFLoader THREE.GLTFLoader]</li>\\n\\t\\t\\t\\t<li>THREE.MMDLoader</li>\\n\\t\\t\\t</ul>\\n\\n\\t\\t<p class=\"desc\">\\n\\t\\t\\t请注意，3ds max和Maya当前无法直接导出多个动画（这意味着动画不是在同一时间线上）到一个文件中。\\n\\t\\t</p>\\n\\n\\t\\t<h2>范例</h2>\\n\\n\\t\\t<code>\\n\\t\\tlet mesh;\\n\\n\\t\\t// 新建一个AnimationMixer, 并取得AnimationClip实例列表\\n\\t\\tconst mixer = new THREE.AnimationMixer( mesh );\\n\\t\\tconst clips = mesh.animations;\\n\\n\\t\\t// 在每一帧中更新mixer\\n\\t\\tfunction update () {\\n\\t\\t\\tmixer.update( deltaSeconds );\\n\\t\\t}\\n\\n\\t\\t// 播放一个特定的动画\\n\\t\\tconst clip = THREE.AnimationClip.findByName( clips, \\'dance\\' );\\n\\t\\tconst action = mixer.clipAction( clip );\\n\\t\\taction.play();\\n\\n\\t\\t// 播放所有动画\\n\\t\\tclips.forEach( function ( clip ) {\\n\\t\\t\\tmixer.clipAction( clip ).play();\\n\\t\\t} );\\n\\t\\t</code>\\n\\n\\t</body>\\n</html>\\n')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pypdf in langchain ouput chain\n",
    "#文本分割\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"/home/zpc/langchain/blibli/data/animation.html\")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header1\"),\n",
    "    (\"h2\", \"Header2\"),\n",
    "    (\"h3\", \"Header3\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "392939af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header1': '动画系统（[name]）'}, page_content='动画系统（[name]）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述'}, page_content='概述'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述'}, page_content='在three.js动画系统中，您可以为模型的各种属性设置动画：\\n\\t\\t\\t[page:SkinnedMesh]（蒙皮和装配模型）的骨骼，morph targets（变形目标），\\n\\t\\t\\t不同的材料属性（颜色，不透明度，布尔运算），可见性和变换。动画属性可以淡入、淡出、交叉淡化和扭曲。\\n\\t\\t\\t在相同或不同物体上同时发生的动画的权重和时间比例的变化可以独立地进行。\\n\\t\\t\\t相同或不同物体的动画也可以同步发生。 为了在一个同构系统中实现所有这一切，\\n\\t\\t\\tthree.js的动画系统[link:https://github.com/mrdoob/three.js/issues/6881 在2015年彻底改变]（注意过时的信息！），\\n\\t\\t\\t它现在有一个与Unity/虚幻4引擎类似的架构。此页面简要阐述了这个系统中的主要组件以及它们如何协同工作。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画片段（Animation Clips）'}, page_content='动画片段（Animation Clips）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画片段（Animation Clips）'}, page_content='如果您已成功导入3D动画对象（无论它是否有骨骼或变形目标或两者皆有都不要紧）——\\n\\t\\t\\t例如使用[link:https://github.com/KhronosGroup/glTF-Blender-IO glTF Blender exporter]（glTF Blender导出器） 从Blender导出它并使用[page:GLTFLoader]将其加载到three.js场景中\\n\\t\\t\\t—— 其中一个响应字段应该是一个名为“animations”的数组，\\n\\t\\t\\t其中包含此模型的[page:AnimationClip AnimationClips]（请参阅下面可用的加载器列表）。 每个*AnimationClip*通常保存对象某个活动的数据。\\n\\t\\t\\t举个例子，假如mesh是一个角色，可能有一个AnimationClip实现步行循环，\\n\\t\\t\\t第二个AnimationClip实现跳跃，第三个AnimationClip实现闪避等等。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '关键帧轨道（Keyframe Tracks）'}, page_content='关键帧轨道（Keyframe Tracks）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '关键帧轨道（Keyframe Tracks）'}, page_content='在这样的*AnimationClip*里面，每个动画属性的数据都存储在一个单独的[page:KeyframeTrack]中。\\n\\t\\t\\t假设一个角色对象有[page:Skeleton]（骨架），\\n\\t\\t\\t一个关键帧轨道可以存储下臂骨骼位置随时间变化的数据，\\n\\t\\t\\t另一个轨道追踪同一块骨骼的旋转变化，第三个追踪另外一块骨骼的位置、转角和尺寸，等等。\\n\\t\\t\\t应该很清楚，AnimationClip可以由许多这样的轨道组成。 假设模型具有morph Targets（变形目标）——\\n\\t\\t\\t例如一个变形目标显示一个笑脸，另一个显示愤怒的脸 ——\\n\\t\\t\\t每个轨道都持有某个变形目标在AnimationClip运行期间产生的[page:Mesh.morphTargetInfluences]（变形目标影响）如何变化的信息。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画混合器（Animation Mixer）'}, page_content='动画混合器（Animation Mixer）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画混合器（Animation Mixer）'}, page_content='存储的数据仅构成动画的基础 —— 实际播放由[page:AnimationMixer]控制。\\n\\t\\t\\t你可以想象这不仅仅是动画的播放器，而是作为硬件的模拟，如真正的调音台，可以同时控制和混合若干动画。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画行为（Animation Actions）'}, page_content='动画行为（Animation Actions）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画行为（Animation Actions）'}, page_content='*AnimationMixer*本身只有很少的（大体上）属性和方法，\\n\\t\\t\\t因为它可以通过[page:AnimationAction AnimationActions]来控制。\\n\\t\\t\\t通过配置*AnimationAction*，您可以决定何时播放、暂停或停止其中一个混合器中的某个*AnimationClip*，\\n\\t\\t\\t这个*AnimationClip*是否需要重复播放以及重复的频率，\\n\\t\\t\\t是否需要使用淡入淡出或时间缩放，以及一些其他内容（例如交叉渐变和同步）。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画对象组（Animation Object Groups）'}, page_content='动画对象组（Animation Object Groups）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '动画对象组（Animation Object Groups）'}, page_content='如果您希望一组对象接收共享的动画状态，则可以使用[page:AnimationObjectGroup]。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '支持的格式和加载器（Supported Formats and Loaders）'}, page_content='支持的格式和加载器（Supported Formats and Loaders）'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '概述', 'Header3': '支持的格式和加载器（Supported Formats and Loaders）'}, page_content='请注意，并非所有模型格式都包含动画（尤其是OBJ，没有），\\n\\t\\t\\t而且只有某些three.js加载器支持[page:AnimationClip AnimationClip]序列。\\n\\t\\t\\t以下几个 支持此动画类型：  \\n确实  \\n[page:ObjectLoader THREE.ObjectLoader]  \\nTHREE.BVHLoader  \\nTHREE.ColladaLoader  \\nTHREE.FBXLoader  \\n[page:GLTFLoader THREE.GLTFLoader]  \\nTHREE.MMDLoader  \\n请注意，3ds max和Maya当前无法直接导出多个动画（这意味着动画不是在同一时间线上）到一个文件中。'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '范例'}, page_content='范例'),\n",
       " Document(metadata={'Header1': '动画系统（[name]）', 'Header2': '范例'}, page_content=\"let mesh;\\n\\n\\t\\t// 新建一个AnimationMixer, 并取得AnimationClip实例列表\\n\\t\\tconst mixer = new THREE.AnimationMixer( mesh );\\n\\t\\tconst clips = mesh.animations;\\n\\n\\t\\t// 在每一帧中更新mixer\\n\\t\\tfunction update () {\\n\\t\\t\\tmixer.update( deltaSeconds );\\n\\t\\t}\\n\\n\\t\\t// 播放一个特定的动画\\n\\t\\tconst clip = THREE.AnimationClip.findByName( clips, 'dance' );\\n\\t\\tconst action = mixer.clipAction( clip );\\n\\t\\taction.play();\\n\\n\\t\\t// 播放所有动画\\n\\t\\tclips.forEach( function ( clip ) {\\n\\t\\t\\tmixer.clipAction( clip ).play();\\n\\t\\t} );\")]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "html_header_splits = html_splitter.split_text(doc[0].page_content)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4095c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"一群科学家带回恐龙爆发了混乱\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"科幻小说\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"故事发生在1920年北洋年间中国南方，马邦德花钱买官，购得“萨南康省”的县长一职，坐“马拉的火车”赴任途中遭马匪张麻子一行人伏击\",\n",
    "        metadata={\"year\": 2010, \"director\": \"姜文\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"话说孙悟空护送唐三藏前往西天取经，半路却和牛魔王合谋要杀害唐三藏，并偷走了紫霞仙子持有的月光宝盒。观音闻讯赶到，欲除掉孙悟空以免危害苍生。唐三藏慈悲为怀，愿意一命赔一命，感化劣徒，观音遂令孙悟空五百年后投胎做人，赎其罪孽。\",\n",
    "        metadata={\"year\": 1994, \"director\": \"刘镇伟\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"故事背景设定在2075年，讲述了太阳即将毁灭，毁灭之后的太阳系已经不适合人类生存，而面对绝境，人类将开启“流浪地球”计划，试图带着地球一起逃离太阳系，寻找人类新家园的故事。\",\n",
    "        metadata={\"year\": 2019, \"director\": \"郭帆\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"该片讲述了耿浩和好哥们郝义一场荒诞而有趣的“寻爱之旅”。该片采用双线叙事的手法，以耿浩和康小雨婚姻破裂为叙事的起点，在郝义携耿浩前往剧组送道具途中“寻爱”的故事中，穿插着昔日康小雨孤身前往大理并与耿浩相遇的前尘往事，讲述着在现代生活中不同人群对婚姻、生活与理想的不同追求。\",\n",
    "        metadata={\"year\": 2014, \"genre\": \"喜剧\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e0d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x7fa7af187dc0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caa3f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description='电影的类型。 [\"科幻小说\",\"喜剧\",\"剧情片\"，\"惊悚片\",\"爱情片\",\"动作片\",\"动画片\"]之一',\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"电影上映年份\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"电影导演的名字\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"电影评分为 1-10\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "document_content_description = \"电影的简要概述\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af43015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    model,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,\n",
    "    search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbf54be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'director': '刘镇伟', 'year': 1994, 'rating': 8.6}, page_content='话说孙悟空护送唐三藏前往西天取经，半路却和牛魔王合谋要杀害唐三藏，并偷走了紫霞仙子持有的月光宝盒。观音闻讯赶到，欲除掉孙悟空以免危害苍生。唐三藏慈悲为怀，愿意一命赔一命，感化劣徒，观音遂令孙悟空五百年后投胎做人，赎其罪孽。')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example only specifies a filter\n",
    "retriever.invoke(\"给我推荐一部评分8.5以上的电影\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade29f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
