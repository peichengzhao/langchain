{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84921270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from openai import api_key, base_url\n",
    "QWEN_API_KEY = os.getenv(\"QWEN_API_KEY\", \"sk-ypjnechwutigvbyglbkguukzsmzkkxfibauydwkbjrypwojd\")\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\", api_key=QWEN_API_KEY, base_url=\"https://api.siliconflow.cn/v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef967610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Âàò‰∫¶Ëè≤ÊúÄËøëÂá†Â§©Âú®ÊàêÈÉΩÂ∑•‰ΩúÔºåÂèÇÂä†ÊüêÂïÜ‰∏öÊ¥ªÂä®„ÄÇÂøôÂÆåÂ∑•‰ΩúÂêéÁöÑÂ•πÂºÄÂêØ‰∫ÜÈÄõË°óË¥≠Áâ©ÁöÑÊ®°ÂºèÔºåÂú®Â§™Âè§ÈáåË¢´ÂÅ∂ÈÅá‰∫Ü„ÄÇ ËØ•ÁΩëÂèãÂ§∏ËµûÂàò‰∫¶Ëè≤Ê∞îË¥®Â§™Â•Ω‰∫ÜÔºÅ', 'ÊúÄËøë‰∏äÊµ∑ËøûÂäû‰∏§Âú∫Â§ßÊ¥ªÂä®Ôºå‰∏Ä‰∏™ÊòØÁôΩÁéâÂÖ∞Â•ñÊèêÂêçÊôöÂÆ¥ÔºåÂè¶‰∏Ä‰∏™ÊòØLVÁöÑÊó∂Ë£ÖÁßÄ„ÄÇÂàò‰∫¶Ëè≤Âõ†‰∏∫Êó∂Èó¥ÂÜ≤Á™ÅÊ≤°ÂéªLVÔºå‰ΩÜÁÉ≠ÊêúÂâçÂçÅÊúâËøë‰∏ÄÂçäÈÉΩÊòØÂ•π„ÄÇLVÁé∞Âú∫Êù•‰∫ÜÂÆãËåú„ÄÅÊú±‰∏ÄÈæôËøô‰∫õÈ°∂ÊµÅÔºå ...', '6Êúà20Êó•ËöÇËöÅ‰øùÈùûÂÖ¨ÂºÄÊ¥ªÂä®ÔºåËè≤ÂêßËé∑Âæó15‰∏™ÂêçÈ¢ùÔºåÁé∞ËøõË°å‰∏çÂêåÊ∏†ÈÅìÁöÑÊãõÂãüÔºåËØ∑Â§ßÂÆ∂Êü•ÁúãÂÆå‰∏ãËø∞ÂøÖÈ°ªÁ¨¶ÂêàÁöÑÊù°‰ª∂ÂêéÔºåÊåëÈÄâËá™Â∑±ÈÄÇÂêàÁöÑÊ∏†ÈÅìËøõË°åÊä•ÂêçÔºö 1„ÄÅÊä•ÂêçËÄÖÂøÖÈ°ªÊòØËöÇËöÅ‰øùÁî®Êà∑‰∏î‚ÄúÊàëÁöÑ ...', 'Ë≥áÁîüÂ†Ç| SHISEIDO ÂÖ®ÁêÉÂìÅÁâå‰ª£Ë®Ä‰∫∫Âäâ‰∫¶Ëè≤x Ë≥áÁîüÂ†ÇÂ∞àË®™ÔºöÊ•µ‰∏äÂæ°ËóèÁ≥ªÂàó Global Brand Ambassador Liu Yifei x Shiseido Exclusive InterviewÔºöFUTURE SOLUTION ‚úì Weibo ...', '... ÊúâÊù®Á¥´„ÄÅÈó´Â¶ÆËøô‰∫õËÄÅÊàèÈ™®Ôºå‰∫∫ÂÆ∂ÊºîÊäÄÁ°ÆÂÆûÊØîÂÅ∂ÂÉèÊ¥æÂº∫ÔºåÂàò‰∫¶Ëè≤ËøòÊîæ‰∫ÜÁôΩÁéâÂÖ∞È¢ÅÂ•ñÁ§ºÁöÑÈ∏ΩÂ≠êÔºå‰πãÂâç‰∏∫‰∫ÜÂéªÊèêÂêçÊôöÂÆ¥Êé®ÊéâLVÊ¥ªÂä®ÔºåÁªìÊûúÈ¢ÅÂ•ñÈÇ£Â§©‰∫∫ÂΩ±ÈÉΩÊ≤°ËßÅÁùÄÔºåËøô‰πü ...', 'Âàò‰∫¶Ëè≤ËøëÊúüÂèÇÂä†‰∫ÜÂ§ö‰∏™Ê¥ªÂä®„ÄÇÂ•πÂá∫Â∏≠ÂÆùÊ†º‰∏ΩÊ¥ªÂä®ÔºåË∫´ÁùÄÈïøË£ôÔºåÊê≠ÈÖçÂÖâÂΩ©Â§∫ÁõÆÁöÑÁè†ÂÆùÔºåÂ∞ΩÊòæË¥µÊ∞îËø∑‰∫∫ÔºåÂÖ∂ÁæéÂú®Ê¥ªÂä®Áé∞Âú∫ÂçÅÂàÜÊä¢ÈïúÔºå‰∏îÂ•πÂú®Ê¥ªÂä®‰∏≠ÁöÑË°®Áé∞Â§ßÊñπÂæó‰ΩìÔºåÊó†ËÆ∫ÊòØËàûÂè∞‰∏äËøòÊòØËàû‰ºöÁéØËäÇÈÉΩ ...', 'ÊõæÁªèÁöÑÂàò‰∫¶Ëè≤Âá≠ÂÄü‚ÄúÁ•û‰ªôÂßêÂßê‚ÄùËøô‰∏™Áß∞Âè∑ÔºåÂ∞±ÂèØ‰ª•Âú®ÂúàÂÜÖËé∑ÂæóÁªùÂØπÁöÑ‰ΩçÁΩÆÔºåÂú®ÂèÇÂä†Áõ∏ÂÖ≥Ê¥ªÂä®Êó∂‰πüËÉΩÂ§üÂæóÂà∞‰∏ªÂäûÊñπÁöÑËÇØÂÆö„ÄÇ ¬∑ Áõ¥Êé•Â∞±Êääc‰ΩçÁªôÂà∞‰∫ÜÂ•πÁöÑÊâã‰∏ä„ÄÇ ¬∑ ‰ªäÂπ¥9Êúà3Êó• ...', 'Âàò‰∫¶Ëè≤Á∫ø‰∏ãËßÅÈù¢‰ºöÊù•ÂíØÔºÅÊ∑±Âú≥ÂêÉÁöÑÂ§™Â•Ω‰∫ÜÂêß„ÄÇÊ¥ªÂä®È°πÁõÆ ÔºöÂàò‰∫¶Ëè≤ÂìÅÁâåÂèëÂ∏É‰ºö¬∑Á∫ø‰∏ãÊ¥ªÂä®Ê¥ªÂä®Êó∂Èó¥üèôÔ∏èÔºö2025Âπ¥4Êúà17Êó•Ê¥ªÂä®Âú∞ÁÇπ‚è∞ÔºöÊ∑±Âú≥Âà∞Âú∫ÂòâÂÆæ ÔºöÂàò‰∫¶Ëè≤Ê¥ªÂä®ÂÜÖÂÆπ ÔºöÁ≠æÂà∞„ÄÅ ...', 'Â•πÁöÑÊúÄÊñ∞Âä®ÊÄÅÔºåÊòØ‰∏∫ÊüêÂ•¢‰æàÂìÅÁâåÁ´ôÂè∞Ôºå‰ªéÊúçË£ÖÂà∞ÈÖçÈ•∞ÂÜçÂà∞ÂèëÂûãÈÉΩÁ≤æÂøÉÊåëÈÄâÊê≠ÈÖçÔºåËÄåËøôÊ¨°Ê¥ªÂä®ËøòÈÇÄËØ∑‰∫ÜÂ•ΩËé±ÂùûÊòéÊòüÂÆâÂ¶Æ¬∑Êµ∑ÁëüËñáÂà∞Âú∫ÔºåÂπ∂ÂíåÂàò‰∫¶Ëè≤‰∫≤ÂØÜ‰∫íÂä®„ÄÇ', '3Êúà19Êó•Ôºå2025‰∏äÊµ∑ÂΩ±ËßÜÂàõÂà∂Â§ß‰ºöÁ≥ªÂàóÊ¥ªÂä®‚Äî‚ÄîÂâßËÄÄ‰∏úÊñπ¬∑2025ÁîµËßÜÂâßÂìÅË¥®ÁõõÂÖ∏Âç≥Â∞Ü‰∏æË°å„ÄÇÊ¥ªÂä®Ê±áËÅö„ÄäÂÖ≠ÂßäÂ¶π„Äã„ÄäÁé´Áë∞ÁöÑÊïÖ‰∫ã„ÄãÁ≠âÂ§öÈÉ®Âπ¥Â∫¶Âè£Á¢ë‰Ω≥‰ΩúÁöÑ‰∏ªÂàõÂõ¢ÈòüÔºåÂ±ïÁé∞ ...']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "SERPAPI_API_KEY = \"001bd195e11581af719e2f10bfb5abd7db5f51ffb37f79b2ef7cd43af0b8e7a2\"\n",
    "search = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)\n",
    "search.run(\"Âàò‰∫¶Ëè≤ÊúÄËøëÊúâ‰ªÄ‰πàÊ¥ªÂä®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5308e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "searchTool = Tool(\n",
    "    name=\"search\",\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    "    func=search.run,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc08774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4fcab92f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 8.94 MiB is free. Process 1942885 has 38.62 GiB memory in use. Process 2129480 has 870.00 MiB memory in use. Of the allocated memory 436.93 MiB is allocated by PyTorch, and 17.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[1;32m      5\u001b[0m embedding_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/zpc/langchain/blibli/bge-large-zh-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/zpc/langchain/blibli/data/faq_1.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m doc \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:223\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     emit_warning()\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:92\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py:1369\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 928 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py:928\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 928\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py:955\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 955\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1350\u001b[0m             device,\n\u001b[1;32m   1351\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1352\u001b[0m             non_blocking,\n\u001b[1;32m   1353\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1354\u001b[0m         )\n\u001b[0;32m-> 1355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 8.94 MiB is free. Process 1942885 has 38.62 GiB memory in use. Process 2129480 has 870.00 MiB memory in use. Of the allocated memory 436.93 MiB is allocated by PyTorch, and 17.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_path = \"/home/zpc/langchain/blibli/bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_path)\n",
    "\n",
    "loader = TextLoader(\"/home/zpc/langchain/blibli/data/faq_1.txt\")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d43623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"/home/zpc/langchain/blibli/data/faq_2.txt\")\n",
    "doc1 = loader.load()\n",
    "loader = TextLoader(\"/home/zpc/langchain/blibli/data/animation.html\")\n",
    "doc2 = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29064c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectorStoreDB \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents([doc[\u001b[38;5;241m0\u001b[39m], doc1[\u001b[38;5;241m0\u001b[39m], doc2[\u001b[38;5;241m0\u001b[39m]],embedding\u001b[38;5;241m=\u001b[39m\u001b[43membeddings\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "\n",
    "vectorStoreDB = FAISS.from_documents([doc[0], doc1[0], doc2[0]],embedding=embeddings)\n",
    "retriver = vectorStoreDB.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "retriver_tool = create_retriever_tool(\n",
    "    retriver,\n",
    "    name=\"retriver\",\n",
    "    \"Âçé‰∏∫ÂïÜÂüéÂ∏ÆÂä©‰∏≠ÂøÉÊñáÊ°£ÊêúÁ¥¢\"\n",
    ")\n",
    "\n",
    "tools = [retriver_tool, searchTool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a76cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver_tool.invoke(\"‰ºóÊµãÊ¥ªÂä®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5a774c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7f971e4b0f70>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7f971e4b0f70>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5137c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "agent = create_openai_functions_agent(llm, tools, verbose=True)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_exexutor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_exexutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8b9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d1e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd8b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff10011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469372d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36316b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b56ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a702921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69883456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d7f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b4b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ce6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d9594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb26cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854da75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce36810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9a333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd695b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
